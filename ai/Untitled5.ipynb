{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192607ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns: ['Latitude', 'Longitude', 'UTC', 'Parameter', 'Unit', 'AQI', 'Category']\n",
      "   Latitude  Longitude               UTC Parameter   Unit  AQI  Category\n",
      "0   37.9722  -122.5189  2025-10-01T06:00     OZONE    PPB   31         1\n",
      "1   37.9722  -122.5189  2025-10-01T06:00     PM2.5  UG/M3    2         1\n",
      "2   37.9722  -122.5189  2025-10-01T06:00       NO2    PPB    0         1\n",
      "3   37.9722  -122.5189  2025-10-01T06:00        CO    PPM -999      -999\n",
      "4   37.7658  -122.3978  2025-10-01T06:00     OZONE    PPB   27         1\n",
      "\n",
      "Normalized preview:\n",
      "                     ts_utc parameter value   unit   aqi  category_code  \\\n",
      "0 2025-10-01 06:00:00+00:00     OZONE  <NA>    PPB  31.0            1.0   \n",
      "1 2025-10-01 06:00:00+00:00     PM2.5  <NA>  UG/M3   2.0            1.0   \n",
      "2 2025-10-01 06:00:00+00:00       NO2  <NA>    PPB   0.0            1.0   \n",
      "3 2025-10-01 06:00:00+00:00        CO  <NA>    PPM   NaN            NaN   \n",
      "4 2025-10-01 06:00:00+00:00     OZONE  <NA>    PPB  27.0            1.0   \n",
      "\n",
      "       lat       lon parameter_std  \n",
      "0  37.9722 -122.5189            o3  \n",
      "1  37.9722 -122.5189          pm25  \n",
      "2  37.9722 -122.5189           no2  \n",
      "3  37.9722 -122.5189            co  \n",
      "4  37.7658 -122.3978            o3  \n",
      "\n",
      "Pivoted (wide) preview:\n",
      "parameter_std                    ts_utc        lat         lon  aqi_no2  \\\n",
      "0             2025-10-01 06:00:00+00:00  37.338202 -121.849892      4.0   \n",
      "1             2025-10-01 06:00:00+00:00  37.348300 -121.894700      1.0   \n",
      "2             2025-10-01 06:00:00+00:00  37.482800 -122.202200      NaN   \n",
      "3             2025-10-01 06:00:00+00:00  37.654700 -122.031700      NaN   \n",
      "4             2025-10-01 06:00:00+00:00  37.689750 -121.771550      NaN   \n",
      "\n",
      "parameter_std  aqi_o3  aqi_pm25  aqi_so2  aqi_now dominant  color  \n",
      "0                 NaN      22.0      NaN     22.0     pm25  Green  \n",
      "1                30.0      41.0      NaN     41.0     pm25  Green  \n",
      "2                 NaN      17.0      NaN     17.0     pm25  Green  \n",
      "3                27.0       NaN      NaN     27.0       o3  Green  \n",
      "4                 NaN      12.0      NaN     12.0     pm25  Green  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API = \"A1EAAE46-87DF-4A9D-9FCA-90A3AFA97D56\"\n",
    "url = \"https://www.airnowapi.org/aq/data/\"\n",
    "\n",
    "params = {\n",
    "    \"startDate\": \"2025-10-01T06\",     # top of hour\n",
    "    \"endDate\":   \"2025-10-01T07\",\n",
    "    \"parameters\": \"PM25,PM10,OZONE,NO2,SO2,CO\",\n",
    "    \"BBOX\": \"-122.60,37.30,-121.70,38.10\",  # lon_min,lat_min,lon_max,lat_max\n",
    "    \"dataType\": \"A\",                   # observations\n",
    "    \"format\": \"application/json\",\n",
    "    \"API_KEY\": API\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params=params, timeout=60)\n",
    "resp.raise_for_status()\n",
    "raw = pd.DataFrame(resp.json())\n",
    "\n",
    "print(\"Raw columns:\", raw.columns.tolist())\n",
    "print(raw.head())\n",
    "\n",
    "# ---------- Normalize columns ----------\n",
    "def _pick(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_airnow(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"ts_utc\",\"parameter\",\"value\",\"unit\",\"aqi\",\"category_code\",\"lat\",\"lon\",\"parameter_std\"\n",
    "        ])\n",
    "\n",
    "    col_utc   = _pick(df, [\"UTC\",\"DateObserved\",\"DateTimeUTC\",\"ValidDate\"])\n",
    "    col_param = _pick(df, [\"Parameter\",\"ParameterName\"])\n",
    "    col_value = _pick(df, [\"Value\",\"Concentration\"])     # often missing in this endpoint\n",
    "    col_unit  = _pick(df, [\"Unit\",\"UnitName\"])\n",
    "    col_aqi   = _pick(df, [\"AQI\",\"AQIValue\"])\n",
    "    col_cat   = _pick(df, [\"Category\",\"CategoryName\"])   # here it's a numeric code\n",
    "    col_lat   = _pick(df, [\"Latitude\",\"lat\",\"Lat\"])\n",
    "    col_lon   = _pick(df, [\"Longitude\",\"lon\",\"Long\"])\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"ts_utc\"]   = pd.to_datetime(df[col_utc], errors=\"coerce\", utc=True) if col_utc else pd.NaT\n",
    "    out[\"parameter\"]= df[col_param] if col_param else pd.NA\n",
    "    out[\"value\"]    = df[col_value] if col_value else pd.NA\n",
    "    out[\"unit\"]     = df[col_unit]  if col_unit  else pd.NA\n",
    "    out[\"aqi\"]      = pd.to_numeric(df[col_aqi], errors=\"coerce\") if col_aqi else pd.NA\n",
    "    out[\"category_code\"] = pd.to_numeric(df[col_cat], errors=\"coerce\") if col_cat else pd.NA\n",
    "    out[\"lat\"]      = pd.to_numeric(df[col_lat], errors=\"coerce\") if col_lat else pd.NA\n",
    "    out[\"lon\"]      = pd.to_numeric(df[col_lon], errors=\"coerce\") if col_lon else pd.NA\n",
    "\n",
    "    # Clean placeholders: -999 means missing\n",
    "    out.loc[out[\"aqi\"] == -999, \"aqi\"] = pd.NA\n",
    "    out.loc[out[\"category_code\"] == -999, \"category_code\"] = pd.NA\n",
    "\n",
    "    # Standardize parameter labels for pivoting\n",
    "    param_map = {\"PM2.5\":\"pm25\",\"PM25\":\"pm25\",\"PM10\":\"pm10\",\"OZONE\":\"o3\",\"O3\":\"o3\",\n",
    "                 \"NO2\":\"no2\",\"SO2\":\"so2\",\"CO\":\"co\"}\n",
    "    out[\"parameter_std\"] = out[\"parameter\"].astype(str).str.upper().map(param_map).fillna(out[\"parameter\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "norm = normalize_airnow(raw)\n",
    "print(\"\\nNormalized preview:\")\n",
    "print(norm.head())\n",
    "\n",
    "# ---------- Pivot safely (index only on columns that actually exist & are not all-NaN) ----------\n",
    "# Use only keys that are present AND not entirely NaN\n",
    "candidate_keys = [\"ts_utc\",\"lat\",\"lon\"]\n",
    "key_cols = [k for k in candidate_keys if k in norm.columns and norm[k].notna().any()]\n",
    "\n",
    "if not key_cols:\n",
    "    raise RuntimeError(\"No valid key columns to pivot on (check payload).\")\n",
    "\n",
    "# Pivot AQI (this endpoint returns AQI per pollutant)\n",
    "aqi_wide = (\n",
    "    norm.pivot_table(index=key_cols, columns=\"parameter_std\", values=\"aqi\", aggfunc=\"max\", dropna=True)\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "# Prefix pollutant columns with 'aqi_'\n",
    "aqi_cols = [c for c in aqi_wide.columns if c not in key_cols]\n",
    "aqi_wide = aqi_wide.rename(columns={c: f\"aqi_{c}\" for c in aqi_cols})\n",
    "\n",
    "# Composite AQI across pollutants (row-wise max)\n",
    "aqi_only = aqi_wide[[c for c in aqi_wide.columns if c.startswith(\"aqi_\")]]\n",
    "aqi_wide[\"aqi_now\"] = aqi_only.max(axis=1, skipna=True)\n",
    "aqi_wide[\"dominant\"] = aqi_only.idxmax(axis=1).str.replace(\"aqi_\",\"\")\n",
    "\n",
    "# Optional: simple color bucket for your map\n",
    "def aqi_color(aqi):\n",
    "    if pd.isna(aqi): return \"Missing\"\n",
    "    aqi = float(aqi)\n",
    "    if aqi <= 100: return \"Green\"     # Good/Moderate\n",
    "    if aqi <= 150: return \"Yellow\"    # Unhealthy for Sensitive Groups\n",
    "    return \"Red\"                      # Unhealthy or worse\n",
    "\n",
    "aqi_wide[\"color\"] = aqi_wide[\"aqi_now\"].apply(aqi_color)\n",
    "\n",
    "print(\"\\nPivoted (wide) preview:\")\n",
    "print(aqi_wide.head())\n",
    "\n",
    "# Save if you want\n",
    "# aqi_wide.to_parquet(\"airnow_window.parquet\", index=False)\n",
    "# aqi_wide.to_csv(\"airnow_window.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f492960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 365 rows to airnow_last_day.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "\n",
    "API = \"A1EAAE46-87DF-4A9D-9FCA-90A3AFA97D56\"\n",
    "# Set your area of interest (San Francisco area example)\n",
    "BBOX = (-122.60, 37.30, -121.70, 38.10)  # (lon_min, lat_min, lon_max, lat_max)\n",
    "PARAMS = (\"PM25\",\"PM10\",\"OZONE\",\"NO2\",\"SO2\",\"CO\")  # request all common pollutants\n",
    "SLEEP_SECS = 0.4  # be nice to the API\n",
    "OUT_CSV = \"airnow_last_day.csv\"\n",
    "\n",
    "def fetch_airnow_slice(start_dt, end_dt, bbox, parameters):\n",
    "    url = \"https://www.airnowapi.org/aq/data/\"\n",
    "    params = {\n",
    "        \"startDate\": start_dt.strftime(\"%Y-%m-%dT%H\"),\n",
    "        \"endDate\":   end_dt.strftime(\"%Y-%m-%dT%H\"),\n",
    "        \"parameters\": \",\".join(parameters),\n",
    "        \"BBOX\": \",\".join(map(str, bbox)),\n",
    "        \"dataType\": \"A\",                   # observations\n",
    "        \"format\": \"application/json\",\n",
    "        \"API_KEY\": API\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return pd.DataFrame(r.json())\n",
    "\n",
    "def _pick(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_airnow(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"ts_utc\",\"parameter\",\"value\",\"unit\",\"aqi\",\n",
    "                                     \"category_code\",\"lat\",\"lon\",\"parameter_std\"])\n",
    "    col_utc  = _pick(df, [\"UTC\",\"DateObserved\",\"DateTimeUTC\",\"ValidDate\"])\n",
    "    col_par  = _pick(df, [\"Parameter\",\"ParameterName\"])\n",
    "    col_val  = _pick(df, [\"Value\",\"Concentration\"])\n",
    "    col_unit = _pick(df, [\"Unit\",\"UnitName\"])\n",
    "    col_aqi  = _pick(df, [\"AQI\",\"AQIValue\"])\n",
    "    col_cat  = _pick(df, [\"Category\",\"CategoryName\"])\n",
    "    col_lat  = _pick(df, [\"Latitude\",\"lat\",\"Lat\"])\n",
    "    col_lon  = _pick(df, [\"Longitude\",\"lon\",\"Long\"])\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"ts_utc\"]   = pd.to_datetime(df[col_utc], errors=\"coerce\", utc=True) if col_utc else pd.NaT\n",
    "    out[\"parameter\"]= df[col_par] if col_par else pd.NA\n",
    "    out[\"value\"]    = df[col_val] if col_val else pd.NA\n",
    "    out[\"unit\"]     = df[col_unit] if col_unit else pd.NA\n",
    "    out[\"aqi\"]      = pd.to_numeric(df[col_aqi], errors=\"coerce\") if col_aqi else pd.NA\n",
    "    out[\"category_code\"] = pd.to_numeric(df[col_cat], errors=\"coerce\") if col_cat else pd.NA\n",
    "    out[\"lat\"]      = pd.to_numeric(df[col_lat], errors=\"coerce\") if col_lat else pd.NA\n",
    "    out[\"lon\"]      = pd.to_numeric(df[col_lon], errors=\"coerce\") if col_lon else pd.NA\n",
    "\n",
    "    # -999 means missing in AirNow\n",
    "    out.loc[out[\"aqi\"] == -999, \"aqi\"] = pd.NA\n",
    "    out.loc[out[\"category_code\"] == -999, \"category_code\"] = pd.NA\n",
    "\n",
    "    # standardize pollutant names for columns\n",
    "    param_map = {\"PM2.5\":\"pm25\",\"PM25\":\"pm25\",\"PM10\":\"pm10\",\"OZONE\":\"o3\",\"O3\":\"o3\",\n",
    "                 \"NO2\":\"no2\",\"SO2\":\"so2\",\"CO\":\"co\"}\n",
    "    out[\"parameter_std\"] = out[\"parameter\"].astype(str).str.upper().map(param_map).fillna(out[\"parameter\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "def build_last_day_csv(bbox, parameters, out_csv, sleep_secs=0.4):\n",
    "    # last 24h window in UTC, top-of-hour\n",
    "    end_dt = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "    start_dt = end_dt - timedelta(hours=24)\n",
    "\n",
    "    # fetch hour by hour\n",
    "    frames = []\n",
    "    t = start_dt\n",
    "    while t < end_dt:\n",
    "        df = fetch_airnow_slice(t, t + timedelta(hours=1), bbox, parameters)\n",
    "        if not df.empty:\n",
    "            frames.append(df)\n",
    "        time.sleep(sleep_secs)\n",
    "        t += timedelta(hours=1)\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No data returned for the last 24 hours.\")\n",
    "        return\n",
    "\n",
    "    raw = pd.concat(frames, ignore_index=True)\n",
    "    norm = normalize_airnow(raw)\n",
    "\n",
    "    # keep only columns needed for pivot\n",
    "    key_cols = [\"ts_utc\",\"lat\",\"lon\"]\n",
    "    aqi_wide = (\n",
    "        norm.pivot_table(index=key_cols, columns=\"parameter_std\", values=\"aqi\", aggfunc=\"max\", dropna=True)\n",
    "            .reset_index()\n",
    "    )\n",
    "\n",
    "    # prefix pollutant columns with 'aqi_'\n",
    "    pol_cols = [c for c in aqi_wide.columns if c not in key_cols]\n",
    "    aqi_wide = aqi_wide.rename(columns={c: f\"aqi_{c}\" for c in pol_cols})\n",
    "\n",
    "    # composite AQI and dominant pollutant\n",
    "    aqi_only = aqi_wide[[c for c in aqi_wide.columns if c.startswith(\"aqi_\")]]\n",
    "    aqi_wide[\"aqi_now\"] = aqi_only.max(axis=1, skipna=True)\n",
    "    aqi_wide[\"dominant\"] = aqi_only.idxmax(axis=1).str.replace(\"aqi_\",\"\")\n",
    "\n",
    "    # sort & save\n",
    "    aqi_wide = aqi_wide.sort_values([\"ts_utc\",\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "    aqi_wide.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {len(aqi_wide):,} rows to {out_csv}\")\n",
    "\n",
    "# --- run it ---\n",
    "build_last_day_csv(BBOX, PARAMS, OUT_CSV, SLEEP_SECS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de06c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITY: New York @ (40.7128, -74.006)\n",
      "[locations] page1 got 57 rows\n",
      "                       ts_utc         parameter parameter_display  \\\n",
      "111 2025-10-03 10:00:00+00:00               pm1               PM1   \n",
      "109 2025-10-03 10:00:00+00:00              pm25             PM2.5   \n",
      "108 2025-10-03 10:00:00+00:00  relativehumidity                RH   \n",
      "113 2025-10-03 10:00:00+00:00       temperature   Temperature (C)   \n",
      "110 2025-10-03 10:00:00+00:00             um003       PM0.3 count   \n",
      "156 2025-10-03 10:00:00+00:00               pm1               PM1   \n",
      "157 2025-10-03 10:00:00+00:00              pm25             PM2.5   \n",
      "158 2025-10-03 10:00:00+00:00  relativehumidity                RH   \n",
      "159 2025-10-03 10:00:00+00:00       temperature   Temperature (C)   \n",
      "160 2025-10-03 10:00:00+00:00             um003       PM0.3 count   \n",
      "84  2025-10-03 10:00:00+00:00               pm1               PM1   \n",
      "89  2025-10-03 10:00:00+00:00              pm25             PM2.5   \n",
      "\n",
      "           value           unit        lat        lon  sensor_id  locationId  \\\n",
      "111     0.888899          µg/m³  40.740022 -73.998504    8520935     2616564   \n",
      "109     1.578869          µg/m³  40.740022 -73.998504    8520925     2616564   \n",
      "108    41.401359              %  40.740022 -73.998504    8520920     2616564   \n",
      "113    17.600000              c  40.740022 -73.998504    8520938     2616564   \n",
      "110   389.876091  particles/cm³  40.740022 -73.998504    8520933     2616564   \n",
      "156     5.616333          µg/m³  40.764337 -73.766338   12212726     3406357   \n",
      "157     8.051514          µg/m³  40.764337 -73.766338   12212727     3406357   \n",
      "158    64.080333              %  40.764337 -73.766338   12212728     3406357   \n",
      "159    13.070667              c  40.764337 -73.766338   12212729     3406357   \n",
      "160  1106.840972  particles/cm³  40.764337 -73.766338   12212730     3406357   \n",
      "84      2.897917          µg/m³  40.585000 -74.238000    7971145     2386747   \n",
      "89      5.006779          µg/m³  40.585000 -74.238000    7815388     2386747   \n",
      "\n",
      "                  location  city country  \n",
      "111  7th Ave and W 16th St  <NA>    <NA>  \n",
      "109  7th Ave and W 16th St  <NA>    <NA>  \n",
      "108  7th Ave and W 16th St  <NA>    <NA>  \n",
      "113  7th Ave and W 16th St  <NA>    <NA>  \n",
      "110  7th Ave and W 16th St  <NA>    <NA>  \n",
      "156      Bayside, NY 11361  <NA>    <NA>  \n",
      "157      Bayside, NY 11361  <NA>    <NA>  \n",
      "158      Bayside, NY 11361  <NA>    <NA>  \n",
      "159      Bayside, NY 11361  <NA>    <NA>  \n",
      "160      Bayside, NY 11361  <NA>    <NA>  \n",
      "84            Carteret, NJ  <NA>    <NA>  \n",
      "89            Carteret, NJ  <NA>    <NA>  \n",
      "[✓] wrote 193 rows → openaq_latest_city_v3_ALL_PARAMS.csv\n"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd, time, math, random\n",
    "from datetime import datetime, timezone\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# =========== SETTINGS ===========\n",
    "DEBUG = True\n",
    "CITY = \"New York\"\n",
    "CITY_COORDS = {\n",
    "    \"San Francisco\": (37.7749, -122.4194),\n",
    "    \"Los Angeles\":   (34.0522, -118.2437),\n",
    "    \"New York\":      (40.7128, -74.0060),\n",
    "    \"Chicago\":       (41.8781, -87.6298),\n",
    "    \"Houston\":       (29.7604, -95.3698),\n",
    "    \"Toronto\":       (43.6532, -79.3832),\n",
    "    \"Vancouver\":     (49.2827, -123.1207),\n",
    "}\n",
    "CENTER_LAT, CENTER_LON = CITY_COORDS[CITY]\n",
    "\n",
    "RADIUS_M = 25000\n",
    "LOC_MAX = 200\n",
    "PER_PAGE = 100\n",
    "SLEEP = 0.15\n",
    "# ⬇️ KEEP EVERYTHING: set to None (no filtering by parameter)\n",
    "POLLUTANTS = None\n",
    "\n",
    "OUT_CSV = \"openaq_latest_city_v3_ALL_PARAMS.csv\"\n",
    "\n",
    "API_KEY = \"e78f079d1ae5390a89f428671ff1349bdefdee203a74f0aff047cee62474901d\"\n",
    "BASE = \"https://api.openaq.org/v3\"\n",
    "HDRS = {\"Accept\": \"application/json\", \"X-API-Key\": API_KEY}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=6, connect=6, read=6, backoff_factor=0.8,\n",
    "        respect_retry_after_header=True,\n",
    "        status_forcelist=[408,429,500,502,503,504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "def get_with_backoff(url, params=None, timeout=30, max_attempts=6):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        r = SESSION.get(url, headers=HDRS, params=params, timeout=timeout)\n",
    "        if r.status_code in (429,500,502,503,504) and attempt < max_attempts:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                sleep_s = float(ra) if ra is not None else min(20.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            except Exception:\n",
    "                sleep_s = min(20.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            if DEBUG:\n",
    "                print(f\"[backoff] {r.status_code} {url} — sleeping {sleep_s:.1f}s (attempt {attempt}/{max_attempts})\")\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "def bbox_from_center(lat, lon, radius_m):\n",
    "    km = radius_m / 1000.0\n",
    "    dlat = km / 111.0\n",
    "    dlon = km / (111.0 * max(0.1, math.cos(math.radians(lat))))\n",
    "    xmin, ymin = lon - dlon, lat - dlat\n",
    "    xmax, ymax = lon + dlon, lat + dlat\n",
    "    return f\"{xmin},{ymin},{xmax},{ymax}\"\n",
    "\n",
    "def norm_param(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, dict):\n",
    "        return str(x.get(\"name\",\"\")).lower() or None\n",
    "    return str(x).lower()\n",
    "\n",
    "def list_locations_bbox(lat, lon, radius_m, per_page=PER_PAGE, limit_locations=LOC_MAX):\n",
    "    url = f\"{BASE}/locations\"\n",
    "    bbox = bbox_from_center(lat, lon, radius_m)\n",
    "    rows, page = [], 1\n",
    "    while len(rows) < limit_locations:\n",
    "        params = {\"bbox\": bbox, \"limit\": per_page, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if DEBUG and page == 1:\n",
    "            print(f\"[locations] page1 got {len(res)} rows\")\n",
    "        if len(res) < per_page: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    df = pd.json_normalize(rows)\n",
    "    keep = [\"id\",\"name\",\"coordinates.latitude\",\"coordinates.longitude\",\"city\",\"country\",\"timezone\"]\n",
    "    for c in keep:\n",
    "        if c not in df.columns: df[c] = pd.NA\n",
    "    df = df.rename(columns={\n",
    "        \"id\":\"locations_id\",\n",
    "        \"coordinates.latitude\":\"lat\",\n",
    "        \"coordinates.longitude\":\"lon\",\n",
    "    })\n",
    "    df = df.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "    df = df[[\"locations_id\",\"name\",\"lat\",\"lon\",\"city\",\"country\",\"timezone\"]].head(limit_locations)\n",
    "    return df\n",
    "\n",
    "def list_sensors_for_location(loc_id):\n",
    "    url = f\"{BASE}/locations/{int(loc_id)}/sensors\"\n",
    "    rows, page = [], 1\n",
    "    while True:\n",
    "        params = {\"limit\": PER_PAGE, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if len(res) < PER_PAGE: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "def fetch_latest_via_location_sensors(center_lat, center_lon, radius_m):\n",
    "    locs = list_locations_bbox(center_lat, center_lon, radius_m)\n",
    "    if locs.empty:\n",
    "        print(\"[i] No locations in bbox.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    flats = []\n",
    "    for _, loc in locs.iterrows():\n",
    "        sensors = list_sensors_for_location(loc[\"locations_id\"])\n",
    "        if sensors.empty:\n",
    "            continue\n",
    "        for _, s in sensors.iterrows():\n",
    "            # parameter (keep ALL)\n",
    "            if \"parameter.name\" in s:\n",
    "                parameter = norm_param(s[\"parameter.name\"])\n",
    "                parameter_display = s.get(\"parameter.displayName\")\n",
    "            else:\n",
    "                parameter = norm_param(s.get(\"parameter\"))\n",
    "                parameter_display = None\n",
    "\n",
    "            # value/unit/timestamp\n",
    "            val = s.get(\"latest.value\")\n",
    "            unit = s.get(\"parameter.units\") or s.get(\"unit\")\n",
    "\n",
    "            ts = (s.get(\"latest.datetime.utc\") or\n",
    "                  s.get(\"latest.datetime.local\") or\n",
    "                  s.get(\"datetimeLast.utc\") or\n",
    "                  s.get(\"datetimeLast.local\"))\n",
    "            ts_utc = pd.to_datetime(ts, errors=\"coerce\", utc=True) if isinstance(ts, str) else pd.NaT\n",
    "\n",
    "            # coords\n",
    "            lat = s.get(\"latest.coordinates.latitude\")\n",
    "            lon = s.get(\"latest.coordinates.longitude\")\n",
    "            if pd.isna(lat) or pd.isna(lon):\n",
    "                lat = loc[\"lat\"]; lon = loc[\"lon\"]\n",
    "\n",
    "            flats.append({\n",
    "                \"ts_utc\": ts_utc,\n",
    "                \"parameter\": parameter,\n",
    "                \"parameter_display\": parameter_display,\n",
    "                \"value\": pd.to_numeric(val, errors=\"coerce\"),\n",
    "                \"unit\": unit,\n",
    "                \"lat\": pd.to_numeric(lat, errors=\"coerce\"),\n",
    "                \"lon\": pd.to_numeric(lon, errors=\"coerce\"),\n",
    "                \"sensor_id\": s.get(\"id\"),\n",
    "                \"locationId\": int(loc[\"locations_id\"]),\n",
    "                \"location\": loc[\"name\"],\n",
    "                \"city\": loc.get(\"city\"),\n",
    "                \"country\": loc.get(\"country\"),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(flats)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.dropna(subset=[\"value\",\"lat\",\"lon\"])\n",
    "    if df[\"ts_utc\"].notna().any():\n",
    "        df = df.sort_values([\"ts_utc\",\"location\",\"parameter\"], ascending=[False, True, True])\n",
    "    else:\n",
    "        df = df.sort_values([\"location\",\"parameter\"])\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    print(f\"CITY: {CITY} @ ({CENTER_LAT}, {CENTER_LON})\")\n",
    "    df = fetch_latest_via_location_sensors(CENTER_LAT, CENTER_LON, RADIUS_M)\n",
    "    if df.empty:\n",
    "        print(\"[i] No latest readings found via v3 location sensors in this radius. Try increasing RADIUS_M.\")\n",
    "        return\n",
    "    if DEBUG:\n",
    "        print(df.head(12))\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[✓] wrote {len(df)} rows → {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42fe767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITY: New York @ (40.7128, -74.006)\n",
      "[locations] page1 got 57 rows\n",
      "                       ts_utc         parameter parameter_display       value  \\\n",
      "111 2025-10-03 10:00:00+00:00               pm1               PM1    0.888899   \n",
      "109 2025-10-03 10:00:00+00:00              pm25             PM2.5    1.578869   \n",
      "108 2025-10-03 10:00:00+00:00  relativehumidity                RH   41.401359   \n",
      "113 2025-10-03 10:00:00+00:00       temperature   Temperature (C)   17.600000   \n",
      "110 2025-10-03 10:00:00+00:00             um003       PM0.3 count  389.876091   \n",
      "45  2025-10-03 10:00:00+00:00                no                NO    0.017000   \n",
      "44  2025-10-03 10:00:00+00:00               no2               NO₂    0.034000   \n",
      "46  2025-10-03 10:00:00+00:00               nox               NOx    0.052000   \n",
      "43  2025-10-03 10:00:00+00:00                o3                O₃    0.001000   \n",
      "156 2025-10-03 10:00:00+00:00               pm1               PM1    5.616333   \n",
      "157 2025-10-03 10:00:00+00:00              pm25             PM2.5    8.051514   \n",
      "158 2025-10-03 10:00:00+00:00  relativehumidity                RH   64.080333   \n",
      "\n",
      "              unit        lat        lon  sensor_id  locationId  \\\n",
      "111          µg/m³  40.740022 -73.998504    8520935     2616564   \n",
      "109          µg/m³  40.740022 -73.998504    8520925     2616564   \n",
      "108              %  40.740022 -73.998504    8520920     2616564   \n",
      "113              c  40.740022 -73.998504    8520938     2616564   \n",
      "110  particles/cm³  40.740022 -73.998504    8520933     2616564   \n",
      "45             ppm  40.670250 -74.126083    4272224        1496   \n",
      "44             ppm  40.670250 -74.126083       2644        1496   \n",
      "46             ppm  40.670250 -74.126083    4272086        1496   \n",
      "43             ppm  40.670250 -74.126083       2645        1496   \n",
      "156          µg/m³  40.764337 -73.766338   12212726     3406357   \n",
      "157          µg/m³  40.764337 -73.766338   12212727     3406357   \n",
      "158              %  40.764337 -73.766338   12212728     3406357   \n",
      "\n",
      "                  location  city country  \n",
      "111  7th Ave and W 16th St  <NA>    <NA>  \n",
      "109  7th Ave and W 16th St  <NA>    <NA>  \n",
      "108  7th Ave and W 16th St  <NA>    <NA>  \n",
      "113  7th Ave and W 16th St  <NA>    <NA>  \n",
      "110  7th Ave and W 16th St  <NA>    <NA>  \n",
      "45                 Bayonne  <NA>    <NA>  \n",
      "44                 Bayonne  <NA>    <NA>  \n",
      "46                 Bayonne  <NA>    <NA>  \n",
      "43                 Bayonne  <NA>    <NA>  \n",
      "156      Bayside, NY 11361  <NA>    <NA>  \n",
      "157      Bayside, NY 11361  <NA>    <NA>  \n",
      "158      Bayside, NY 11361  <NA>    <NA>  \n",
      "[✓] wrote long: 193 rows → openaq_latest_city_v3_ALL_PARAMS.csv\n",
      "[✓] wide shape: (56, 31)\n",
      "        lat       lon                    ts_utc  locationId  \\\n",
      "0  40.58500 -74.23800 2025-10-03 10:00:00+00:00     2386747   \n",
      "1  40.58875 -73.98382 2025-10-03 10:00:00+00:00     3041962   \n",
      "2  40.59690 -74.12640 2017-11-15 22:00:00+00:00         386   \n",
      "3  40.60394 -74.27618 2025-10-03 10:00:00+00:00         924   \n",
      "4  40.63306 -74.13716 2025-10-03 10:00:00+00:00      496096   \n",
      "5  40.64144 -74.20837 2025-10-03 10:00:00+00:00         971   \n",
      "6  40.64182 -74.01871 2025-10-03 10:00:00+00:00         648   \n",
      "7  40.64751 -73.97403 2025-10-03 10:00:00+00:00     4811604   \n",
      "8  40.66239 -74.21481 2024-04-05 11:00:00+00:00        2193   \n",
      "9  40.67025 -74.12608 2025-10-03 10:00:00+00:00        1496   \n",
      "\n",
      "                             location  city country   co     no    no2  ...  \\\n",
      "0                        Carteret, NJ  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "1                      Near Bay 50 St  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "2                        Susan Wagner  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "3                           Rahway PM  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "4                       Port Richmond  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "5                   Elizabeth Trailer  <NA>    <NA>  0.3  0.025  0.025  ...   \n",
      "6                      Bklyn - PS 314  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "7  Caton Ave and Ocean Pkwy, Brooklyn  <NA>    <NA>  NaN    NaN    NaN  ...   \n",
      "8                           Elizabeth  <NA>    <NA>  0.6    NaN    NaN  ...   \n",
      "9                             Bayonne  <NA>    <NA>  NaN  0.017  0.034  ...   \n",
      "\n",
      "   no_unit  nox_unit  o3_unit  pm10_unit  pm1_unit  pm25_unit  \\\n",
      "0      NaN       NaN      NaN      µg/m³     µg/m³      µg/m³   \n",
      "1      NaN       NaN      NaN      µg/m³     µg/m³      µg/m³   \n",
      "2      NaN       NaN      ppm        NaN       NaN        NaN   \n",
      "3      NaN       NaN      NaN        NaN       NaN      µg/m³   \n",
      "4      NaN       NaN      NaN        NaN       NaN      µg/m³   \n",
      "5      ppm       ppm      NaN        NaN       NaN      µg/m³   \n",
      "6      NaN       NaN      NaN        NaN       NaN      µg/m³   \n",
      "7      NaN       NaN      NaN        NaN     µg/m³      µg/m³   \n",
      "8      NaN       NaN      NaN        NaN       NaN        NaN   \n",
      "9      ppm       ppm      ppm        NaN       NaN        NaN   \n",
      "\n",
      "   relativehumidity_unit  so2_unit  temperature_unit     um003_unit  \n",
      "0                      %       NaN                 c  particles/cm³  \n",
      "1                      %       NaN                 c  particles/cm³  \n",
      "2                    NaN       NaN               NaN            NaN  \n",
      "3                    NaN       NaN               NaN            NaN  \n",
      "4                    NaN       NaN               NaN            NaN  \n",
      "5                    NaN       ppm               NaN            NaN  \n",
      "6                    NaN       NaN               NaN            NaN  \n",
      "7                      %       NaN                 c  particles/cm³  \n",
      "8                    NaN       ppm               NaN            NaN  \n",
      "9                    NaN       ppm               NaN            NaN  \n",
      "\n",
      "[10 rows x 31 columns]\n",
      "[✓] wrote wide → openaq_latest_city_v3_ALL_PARAMS_wide.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6760\\2927739776.py:219: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  latest[\"param_unit\"] = latest[\"parameter\"] + \"_unit\"\n"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd, time, math, random\n",
    "from datetime import datetime, timezone\n",
    "from urllib3.util.retry import Retry      # ✅ correct path\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# =========== SETTINGS ===========\n",
    "DEBUG = True\n",
    "CITY = \"New York\"\n",
    "CITY_COORDS = {\n",
    "    \"San Francisco\": (37.7749, -122.4194),\n",
    "    \"Los Angeles\":   (34.0522, -118.2437),\n",
    "    \"New York\":      (40.7128, -74.0060),\n",
    "    \"Chicago\":       (41.8781, -87.6298),\n",
    "    \"Houston\":       (29.7604, -95.3698),\n",
    "    \"Toronto\":       (43.6532, -79.3832),\n",
    "    \"Vancouver\":     (49.2827, -123.1207),\n",
    "}\n",
    "CENTER_LAT, CENTER_LON = CITY_COORDS[CITY]\n",
    "\n",
    "RADIUS_M = 25000\n",
    "LOC_MAX = 200\n",
    "PER_PAGE = 100\n",
    "SLEEP = 0.15\n",
    "# ⬇️ KEEP EVERYTHING: set to None (no filtering by parameter)\n",
    "POLLUTANTS = None\n",
    "\n",
    "OUT_CSV = \"openaq_latest_city_v3_ALL_PARAMS.csv\"        # long\n",
    "OUT_CSV_WIDE = \"openaq_latest_city_v3_ALL_PARAMS_wide.csv\"  # wide (one row per lat,lon)\n",
    "COORD_DECIMALS = 5        # rounding used to merge very-close coordinates\n",
    "INCLUDE_UNIT_COLS = True  # add a column per parameter with suffix _unit\n",
    "\n",
    "API_KEY = \"e78f079d1ae5390a89f428671ff1349bdefdee203a74f0aff047cee62474901d\"\n",
    "BASE = \"https://api.openaq.org/v3\"\n",
    "HDRS = {\"Accept\": \"application/json\", \"X-API-Key\": API_KEY}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=6, connect=6, read=6, backoff_factor=0.8,\n",
    "        respect_retry_after_header=True,\n",
    "        status_forcelist=[408,429,500,502,503,504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "def get_with_backoff(url, params=None, timeout=30, max_attempts=6):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        r = SESSION.get(url, headers=HDRS, params=params, timeout=timeout)\n",
    "        if r.status_code in (429,500,502,503,504) and attempt < max_attempts:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                sleep_s = float(ra) if ra is not None else min(20.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            except Exception:\n",
    "                sleep_s = min(20.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            if DEBUG:\n",
    "                print(f\"[backoff] {r.status_code} {url} — sleeping {sleep_s:.1f}s (attempt {attempt}/{max_attempts})\")\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "def bbox_from_center(lat, lon, radius_m):\n",
    "    km = radius_m / 1000.0\n",
    "    dlat = km / 111.0\n",
    "    dlon = km / (111.0 * max(0.1, math.cos(math.radians(lat))))\n",
    "    xmin, ymin = lon - dlon, lat - dlat\n",
    "    xmax, ymax = lon + dlon, lat + dlat\n",
    "    return f\"{xmin},{ymin},{xmax},{ymax}\"\n",
    "\n",
    "def norm_param(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, dict):\n",
    "        return str(x.get(\"name\",\"\")).lower() or None\n",
    "    return str(x).lower()\n",
    "\n",
    "def list_locations_bbox(lat, lon, radius_m, per_page=PER_PAGE, limit_locations=LOC_MAX):\n",
    "    url = f\"{BASE}/locations\"\n",
    "    bbox = bbox_from_center(lat, lon, radius_m)\n",
    "    rows, page = [], 1\n",
    "    while len(rows) < limit_locations:\n",
    "        params = {\"bbox\": bbox, \"limit\": per_page, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if DEBUG and page == 1:\n",
    "            print(f\"[locations] page1 got {len(res)} rows\")\n",
    "        if len(res) < per_page: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    df = pd.json_normalize(rows)\n",
    "    keep = [\"id\",\"name\",\"coordinates.latitude\",\"coordinates.longitude\",\"city\",\"country\",\"timezone\"]\n",
    "    for c in keep:\n",
    "        if c not in df.columns: df[c] = pd.NA\n",
    "    df = df.rename(columns={\n",
    "        \"id\":\"locations_id\",\n",
    "        \"coordinates.latitude\":\"lat\",\n",
    "        \"coordinates.longitude\":\"lon\",\n",
    "    })\n",
    "    df = df.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "    df = df[[\"locations_id\",\"name\",\"lat\",\"lon\",\"city\",\"country\",\"timezone\"]].head(limit_locations)\n",
    "    return df\n",
    "\n",
    "def list_sensors_for_location(loc_id):\n",
    "    url = f\"{BASE}/locations/{int(loc_id)}/sensors\"\n",
    "    rows, page = [], 1\n",
    "    while True:\n",
    "        params = {\"limit\": PER_PAGE, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if len(res) < PER_PAGE: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "def fetch_latest_via_location_sensors(center_lat, center_lon, radius_m):\n",
    "    locs = list_locations_bbox(center_lat, center_lon, radius_m)\n",
    "    if locs.empty:\n",
    "        print(\"[i] No locations in bbox.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    flats = []\n",
    "    for _, loc in locs.iterrows():\n",
    "        sensors = list_sensors_for_location(loc[\"locations_id\"])\n",
    "        if sensors.empty:\n",
    "            continue\n",
    "        for _, s in sensors.iterrows():\n",
    "            # parameter (keep ALL)\n",
    "            if \"parameter.name\" in s:\n",
    "                parameter = norm_param(s[\"parameter.name\"])\n",
    "                parameter_display = s.get(\"parameter.displayName\")\n",
    "            else:\n",
    "                parameter = norm_param(s.get(\"parameter\"))\n",
    "                parameter_display = None\n",
    "\n",
    "            # optional filter\n",
    "            if POLLUTANTS and parameter not in POLLUTANTS:\n",
    "                continue\n",
    "\n",
    "            # value/unit/timestamp\n",
    "            val = s.get(\"latest.value\")\n",
    "            unit = s.get(\"parameter.units\") or s.get(\"unit\")\n",
    "\n",
    "            ts = (s.get(\"latest.datetime.utc\") or\n",
    "                  s.get(\"latest.datetime.local\") or\n",
    "                  s.get(\"datetimeLast.utc\") or\n",
    "                  s.get(\"datetimeLast.local\"))\n",
    "            ts_utc = pd.to_datetime(ts, errors=\"coerce\", utc=True) if isinstance(ts, str) else pd.NaT\n",
    "\n",
    "            # coords\n",
    "            lat = s.get(\"latest.coordinates.latitude\")\n",
    "            lon = s.get(\"latest.coordinates.longitude\")\n",
    "            if pd.isna(lat) or pd.isna(lon):\n",
    "                lat = loc[\"lat\"]; lon = loc[\"lon\"]\n",
    "\n",
    "            flats.append({\n",
    "                \"ts_utc\": ts_utc,\n",
    "                \"parameter\": parameter,\n",
    "                \"parameter_display\": parameter_display,\n",
    "                \"value\": pd.to_numeric(val, errors=\"coerce\"),\n",
    "                \"unit\": unit,\n",
    "                \"lat\": pd.to_numeric(lat, errors=\"coerce\"),\n",
    "                \"lon\": pd.to_numeric(lon, errors=\"coerce\"),\n",
    "                \"sensor_id\": s.get(\"id\"),\n",
    "                \"locationId\": int(loc[\"locations_id\"]),\n",
    "                \"location\": loc[\"name\"],\n",
    "                \"city\": loc.get(\"city\"),\n",
    "                \"country\": loc.get(\"country\"),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(flats)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.dropna(subset=[\"value\",\"lat\",\"lon\"])\n",
    "    if df[\"ts_utc\"].notna().any():\n",
    "        df = df.sort_values([\"ts_utc\",\"location\",\"parameter\"], ascending=[False, True, True])\n",
    "    else:\n",
    "        df = df.sort_values([\"location\",\"parameter\"])\n",
    "    return df\n",
    "\n",
    "# ---- Pivot to one row per (lat, lon) with parameter columns ----\n",
    "def to_wide_by_point(df: pd.DataFrame, decimals: int = COORD_DECIMALS, include_units: bool = INCLUDE_UNIT_COLS) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"lat_r\"] = d[\"lat\"].round(decimals)\n",
    "    d[\"lon_r\"] = d[\"lon\"].round(decimals)\n",
    "\n",
    "    # normalize time for ordering; missing timestamps go very old\n",
    "    ts = pd.to_datetime(d.get(\"ts_utc\"), errors=\"coerce\", utc=True)\n",
    "    d[\"_ts\"] = ts.fillna(pd.Timestamp(\"1970-01-01\", tz=\"UTC\"))\n",
    "\n",
    "    # keep latest per (point, parameter)\n",
    "    d = d.sort_values(\"_ts\")\n",
    "    latest = d.groupby([\"lat_r\",\"lon_r\",\"parameter\"], as_index=False).tail(1)\n",
    "\n",
    "    # representative timestamp per point\n",
    "    point_ts = latest.groupby([\"lat_r\",\"lon_r\"], as_index=False)[\"_ts\"].max().rename(columns={\"_ts\":\"ts_point\"})\n",
    "\n",
    "    # optional representative metadata (mode / most recent)\n",
    "    meta_cols = []\n",
    "    for mc in [\"locationId\",\"location\",\"city\",\"country\"]:\n",
    "        if mc in latest.columns:\n",
    "            meta_cols.append(mc)\n",
    "\n",
    "    # pivot values\n",
    "    wide_val = latest.pivot_table(index=[\"lat_r\",\"lon_r\"], columns=\"parameter\", values=\"value\", aggfunc=\"first\")\n",
    "\n",
    "    if include_units:\n",
    "        latest[\"param_unit\"] = latest[\"parameter\"] + \"_unit\"\n",
    "        wide_unit = latest.pivot_table(index=[\"lat_r\",\"lon_r\"], columns=\"param_unit\", values=\"unit\", aggfunc=\"first\")\n",
    "        wide = pd.concat([wide_val, wide_unit], axis=1)\n",
    "    else:\n",
    "        wide = wide_val\n",
    "\n",
    "    wide = wide.merge(point_ts, left_index=True, right_on=[\"lat_r\",\"lon_r\"], how=\"left\")\n",
    "\n",
    "    # attach metadata\n",
    "    for mc in meta_cols:\n",
    "        meta = (latest.groupby([\"lat_r\",\"lon_r\"])[mc]\n",
    "                    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[-1]))\n",
    "        wide = wide.join(meta, on=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "    # finalize columns/order\n",
    "    wide = wide.reset_index(drop=True)\n",
    "    param_cols = sorted([c for c in wide.columns if c not in {\"lat_r\",\"lon_r\",\"ts_point\",\"locationId\",\"location\",\"city\",\"country\"} and not str(c).endswith(\"_unit\")])\n",
    "    unit_cols  = sorted([c for c in wide.columns if str(c).endswith(\"_unit\")])\n",
    "    ordered = ([\"lat_r\",\"lon_r\",\"ts_point\",\"locationId\",\"location\",\"city\",\"country\"] +\n",
    "               param_cols + unit_cols)\n",
    "    ordered = [c for c in ordered if c in wide.columns]\n",
    "    wide = wide[ordered].rename(columns={\"lat_r\":\"lat\",\"lon_r\":\"lon\",\"ts_point\":\"ts_utc\"})\n",
    "    return wide\n",
    "\n",
    "def main():\n",
    "    print(f\"CITY: {CITY} @ ({CENTER_LAT}, {CENTER_LON})\")\n",
    "    df = fetch_latest_via_location_sensors(CENTER_LAT, CENTER_LON, RADIUS_M)\n",
    "    if df.empty:\n",
    "        print(\"[i] No latest readings found via v3 location sensors in this radius. Try increasing RADIUS_M.\")\n",
    "        return\n",
    "\n",
    "    if DEBUG:\n",
    "        print(df.head(12))\n",
    "\n",
    "    # save long\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[✓] wrote long: {len(df)} rows → {OUT_CSV}\")\n",
    "\n",
    "    # save wide (one row per lat,lon)\n",
    "    wide = to_wide_by_point(df)\n",
    "    print(f\"[✓] wide shape: {wide.shape}\")\n",
    "    if DEBUG:\n",
    "        print(wide.head(10))\n",
    "    wide.to_csv(OUT_CSV_WIDE, index=False)\n",
    "    print(f\"[✓] wrote wide → {OUT_CSV_WIDE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65523fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Earthdata Login username (saved locally in netrc):\n",
      "Username: mahdinasa\n",
      "Password (input hidden): ········\n",
      "[✓] Wrote credentials to C:\\Users\\Lenovo\\_netrc\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6c06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Opening SLV...\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/09/MERRA2_400.t1nxslv.20250926.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/09/MERRA2_400.t1nxslv.20250927.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/09/MERRA2_400.t1nxslv.20250928.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/09/MERRA2_400.t1nxslv.20250929.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/09/MERRA2_400.t1nxslv.20250930.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/10/MERRA2_400.t1nxslv.20251001.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/10/MERRA2_400.t1nxslv.20251002.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2025/10/MERRA2_400.t1nxslv.20251003.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[i] Opening AER...\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/09/MERRA2_400.t1nxaer.20250926.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/09/MERRA2_400.t1nxaer.20250927.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/09/MERRA2_400.t1nxaer.20250928.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/09/MERRA2_400.t1nxaer.20250929.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/09/MERRA2_400.t1nxaer.20250930.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/10/MERRA2_400.t1nxaer.20251001.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/10/MERRA2_400.t1nxaer.20251002.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[skip] could not open https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXAER.5.12.4/2025/10/MERRA2_400.t1nxaer.20251003.nc4: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'pydap']. But their dependencies may not be installed, see:\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html \n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "[!] Nothing opened — check GES DISC authorization, dates, or bbox.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xarray\\backends\\plugins.py:168: RuntimeWarning: 'h5netcdf' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xarray\\backends\\plugins.py:168: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631c8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "748b43c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xarray in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2023.6.0)\n",
      "Collecting xarray\n",
      "  Obtaining dependency information for xarray from https://files.pythonhosted.org/packages/0e/a7/6eeb32e705d510a672f74135f538ad27f87f3d600845bfd3834ea3a77c7e/xarray-2025.9.1-py3-none-any.whl.metadata\n",
      "  Downloading xarray-2025.9.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting netCDF4\n",
      "  Obtaining dependency information for netCDF4 from https://files.pythonhosted.org/packages/cf/ba/d26e8278ad8a2306580bab076b6d64cd16459a60e632e6c1a9cbb68dd3d9/netCDF4-1.7.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading netCDF4-1.7.2-cp311-cp311-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting pydap\n",
      "  Obtaining dependency information for pydap from https://files.pythonhosted.org/packages/3b/61/df917dc9e5190fc3498741710e2143c02aef458f232c69c6a790107a4beb/pydap-3.5.7-py3-none-any.whl.metadata\n",
      "  Downloading pydap-3.5.7-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting cftime\n",
      "  Obtaining dependency information for cftime from https://files.pythonhosted.org/packages/79/b1/6551603f8ea31de55913c84e4def3c36670563bdea6e195fcc4b6225ddf7/cftime-1.6.4.post1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cftime-1.6.4.post1-cp311-cp311-win_amd64.whl.metadata (8.9 kB)\n",
      "Collecting numpy>=1.26 (from xarray)\n",
      "  Obtaining dependency information for numpy>=1.26 from https://files.pythonhosted.org/packages/0a/a2/a4f78cb2241fe5664a22a10332f2be886dcdea8784c9f6a01c272da9b426/numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.9 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 641.7 kB/s eta 0:00:00\n",
      "Collecting packaging>=24.1 (from xarray)\n",
      "  Obtaining dependency information for packaging>=24.1 from https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl.metadata\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pandas>=2.2 (from xarray)\n",
      "  Obtaining dependency information for pandas>=2.2 from https://files.pythonhosted.org/packages/8e/59/712db1d7040520de7a4965df15b774348980e6df45c129b8c64d0dbe74ef/pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from netCDF4) (2023.7.22)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydap) (2.31.0)\n",
      "Collecting requests-cache (from pydap)\n",
      "  Obtaining dependency information for requests-cache from https://files.pythonhosted.org/packages/4e/2e/8f4051119f460cfc786aa91f212165bb6e643283b533db572d7b33952bd2/requests_cache-1.2.1-py3-none-any.whl.metadata\n",
      "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydap) (1.11.1)\n",
      "Collecting Webob (from pydap)\n",
      "  Obtaining dependency information for Webob from https://files.pythonhosted.org/packages/50/bd/c336448be43d40be28e71f2e0f3caf7ccb28e2755c58f4c02c065bfe3e8e/WebOb-1.8.9-py2.py3-none-any.whl.metadata\n",
      "  Downloading WebOb-1.8.9-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydap) (4.12.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydap) (4.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=2.2->xarray) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=2.2->xarray) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=2.2->xarray) (2023.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->pydap) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pydap) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pydap) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->pydap) (1.26.16)\n",
      "Requirement already satisfied: attrs>=21.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests-cache->pydap) (22.1.0)\n",
      "Collecting cattrs>=22.2 (from requests-cache->pydap)\n",
      "  Obtaining dependency information for cattrs>=22.2 from https://files.pythonhosted.org/packages/20/a5/b3771ac30b590026b9d721187110194ade05bfbea3d98b423a9cafd80959/cattrs-25.2.0-py3-none-any.whl.metadata\n",
      "  Downloading cattrs-25.2.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests-cache->pydap) (3.10.0)\n",
      "Collecting url-normalize>=1.4 (from requests-cache->pydap)\n",
      "  Obtaining dependency information for url-normalize>=1.4 from https://files.pythonhosted.org/packages/bc/d9/5ec15501b675f7bc07c5d16aa70d8d778b12375686b6efd47656efdc67cd/url_normalize-2.2.1-py3-none-any.whl.metadata\n",
      "  Downloading url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numpy>=1.26 (from xarray)\n",
      "  Obtaining dependency information for numpy>=1.26 from https://files.pythonhosted.org/packages/3f/6b/5610004206cf7f8e7ad91c5a85a8c71b2f2f8051a0c0c4d5916b76d6cbb2/numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting attrs>=21.2 (from requests-cache->pydap)\n",
      "  Obtaining dependency information for attrs>=21.2 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from cattrs>=22.2->requests-cache->pydap)\n",
      "  Obtaining dependency information for typing-extensions>=4.12.2 from https://files.pythonhosted.org/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2->xarray) (1.16.0)\n",
      "Downloading xarray-2025.9.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.4 MB 871.5 kB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/1.4 MB 762.6 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.2/1.4 MB 1.4 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.3/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.9/1.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.3/1.4 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading netCDF4-1.7.2-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.0 MB 3.6 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/7.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/7.0 MB 3.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/7.0 MB 3.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/7.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 3.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.0 MB 3.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.1/7.0 MB 3.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.3/7.0 MB 3.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.4/7.0 MB 3.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.5/7.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/7.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.7/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.9/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.1/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.8/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.0/7.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.1/7.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/7.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/7.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/7.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.3/7.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/7.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/7.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.4/7.0 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.4/7.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.5/7.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.5/7.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.5/7.0 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.5/7.0 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.5/7.0 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.6/7.0 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.6/7.0 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.8/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.9/7.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.0/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.2/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.2/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.3/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.4/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.5/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.5/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.5/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.8/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.8/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.9/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.1/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.3/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.3/7.0 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.4/7.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/7.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.6/7.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.6/7.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.7/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.0 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.8/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.9/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.1/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.2/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.2/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.3/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.4/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.5/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.9/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading pydap-3.5.7-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.4 MB 1.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/2.4 MB 1.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.1/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.0/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading cftime-1.6.4.post1-cp311-cp311-win_amd64.whl (190 kB)\n",
      "   ---------------------------------------- 0.0/190.2 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 112.6/190.2 kB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 153.6/190.2 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 190.2/190.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.5 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 61.4/66.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.5/66.5 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.3 MB 3.4 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/11.3 MB 1.8 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.3/11.3 MB 2.0 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/11.3 MB 1.9 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/11.3 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/11.3 MB 1.9 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.6/11.3 MB 1.9 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/11.3 MB 2.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/11.3 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/11.3 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/11.3 MB 1.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/11.3 MB 1.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/11.3 MB 1.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.1/11.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.2/11.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.2/11.3 MB 1.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.2/11.3 MB 1.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 1.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 1.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.4/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.4/11.3 MB 1.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.6/11.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.6/11.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.7/11.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.7/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.7/11.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.7/11.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.9/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.9/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.9/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.9/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.9/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.0/11.3 MB 1.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.1/11.3 MB 1.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.1/11.3 MB 1.0 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.2/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.2/11.3 MB 1.0 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.3/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.4/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.4/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.5/11.3 MB 1.1 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.6/11.3 MB 1.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.6/11.3 MB 1.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.7/11.3 MB 1.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.8/11.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.0/11.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.0/11.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.2/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.3/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.4/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.4/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.5/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.5/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.6/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.6/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.7/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.8/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.9/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 4.0/11.3 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.0/11.3 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.1/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.2/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.3/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.4/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.5/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.5/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 4.6/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 4.7/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 4.8/11.3 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 4.8/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.9/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.0/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.0/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.1/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.2/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.2/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.3/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.4/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.5/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.5/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.5/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.6/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.7/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.7/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.8/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.8/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.8/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.9/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.9/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.9/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.0/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.0/11.3 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.1/11.3 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.1/11.3 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.2/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.5/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.5/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.7/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.7/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.8/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.9/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.0/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.0/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.1/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.2/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.2/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.3/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.4/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.5/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.6/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.7/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.7/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.8/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.8/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.9/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.0/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.2/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.3/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.5/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.6/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.6/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.8/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.8/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.0/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.1/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.2/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.3/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.4/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.5/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.6/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.6/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.8/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.9/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.9/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.0/11.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.1/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.3/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.3/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.4/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.6/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.8/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.8/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/61.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.4/61.4 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.8 MB 1.4 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 0.1/15.8 MB 1.4 MB/s eta 0:00:11\n",
      "   ---------------------------------------- 0.2/15.8 MB 1.4 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.3/15.8 MB 1.6 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.4/15.8 MB 1.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/15.8 MB 1.7 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.6/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.6/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.8/15.8 MB 1.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.9/15.8 MB 1.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.0/15.8 MB 1.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.1/15.8 MB 1.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.2/15.8 MB 1.8 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.3/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.3/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.3/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.4/15.8 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.4/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.4/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.6/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.6/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.6/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.7/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.8/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.8/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.8/15.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.9/15.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.0/15.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.2/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.3/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.4/15.8 MB 1.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.5/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.5/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.7/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.8/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.8/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.9/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.0/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.0/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.1/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.2/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.3/15.8 MB 1.5 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.5/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 3.5/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.6/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.7/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.8/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.8/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.9/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.9/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.9/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.0/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.3/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.3/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 4.4/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 4.5/15.8 MB 1.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 4.6/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.8/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.8/15.8 MB 1.6 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.9/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.0/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.2/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.3/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.4/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.4/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.6/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.7/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.7/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.8/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.8/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 6.0/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 6.0/15.8 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 6.1/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 6.2/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 6.2/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 6.3/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.4/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.4/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.5/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.6/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.6/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.7/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.7/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 6.8/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 6.9/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 6.9/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 7.0/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 7.1/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.1/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.2/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.2/15.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.2/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.3/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.4/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.4/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.5/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.5/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.5/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.6/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.7/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.7/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.8/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.9/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 7.9/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.0/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.0/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.1/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.2/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.2/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 8.3/15.8 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 8.4/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.4/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.5/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.6/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.7/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.7/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.8/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.8/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.9/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 9.0/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 9.1/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 9.1/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 9.2/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.5/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.6/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.7/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.8/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.8/15.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.9/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.0/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.0/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.1/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.2/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.3/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.3/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.4/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.4/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.5/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.5/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.5/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.5/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.7/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.7/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.8/15.8 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.8/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.8/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.9/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 10.9/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 11.0/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 11.0/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 11.0/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.1/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.1/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.3/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.3/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.4/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.4/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.5/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.5/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.6/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.6/15.8 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.7/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.7/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.9/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.9/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.0/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.0/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.0/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.0/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.1/15.8 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.1/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.1/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.2/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.2/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.2/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.3/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.4/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.4/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.5/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.5/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.5/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.6/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.7/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.7/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.7/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.8/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.8/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.8/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.8/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.9/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.9/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 13.0/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 13.0/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 13.1/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 13.1/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 13.2/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 13.2/15.8 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 13.3/15.8 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.3/15.8 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.4/15.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.4/15.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.6/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.7/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.7/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.8/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.8/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.8/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.9/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.9/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.9/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.1/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.1/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.1/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.3/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.3/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.3/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.4/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.4/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.5/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.6/15.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.6/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.8/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.8/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.8/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.8/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.0/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.3/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.3/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.4/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading WebOb-1.8.9-py2.py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.4 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 41.0/115.4 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 41.0/115.4 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 115.4/115.4 kB 964.4 kB/s eta 0:00:00\n",
      "Downloading cattrs-25.2.0-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.0 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 41.0/70.0 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 70.0/70.0 kB 766.7 kB/s eta 0:00:00\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/44.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.6/44.6 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: Webob, url-normalize, typing-extensions, packaging, numpy, attrs, pandas, cftime, cattrs, xarray, requests-cache, netCDF4, pydap\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accès refusé: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-5oc1vcpu\\\\core\\\\_multiarray_tests.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4faccc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6835a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITY: New York @ (40.7128, -74.006)\n",
      "Window (UTC): 2025-09-04 11:57:26  →  2025-10-04 11:57:26\n",
      "[locations] page1 got 57 rows\n",
      "[guard] stop sensor 673 at page 9/9 for 720h window\n",
      "[guard] stop sensor 671 at page 9/9 for 720h window\n",
      "[guard] stop sensor 674 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1097 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1102 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1099 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1098 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1103 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1152 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1106 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1121 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1128 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1143 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1145 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1146 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1147 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1522 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1523 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1534 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1535 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1536 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272431 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272250 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1693 at page 9/9 for 720h window\n",
      "[guard] stop sensor 5077566 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1758 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3950 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3951 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3952 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272273 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272290 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3639 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3638 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3640 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1761 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3637 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1777 at page 9/9 for 720h window\n",
      "[guard] stop sensor 23341 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2016 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2018 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272080 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272352 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2646 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2645 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2644 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272224 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272086 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3954 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3953 at page 9/9 for 720h window\n",
      "[guard] stop sensor 25520 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1662910 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2165515 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2189530 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2780575 at page 9/9 for 720h window\n",
      "[guard] stop sensor 6530280 at page 9/9 for 720h window\n",
      "[guard] stop sensor 6530283 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970982 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971373 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972608 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7324143 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972609 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7324145 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971073 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971082 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972670 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972707 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7386938 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7386942 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970916 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970930 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7762999 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7763001 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971870 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971901 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971145 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971292 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971629 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971668 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7815388 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7815389 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971262 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971316 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972696 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972703 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897332 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897339 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970808 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971247 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897354 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897355 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972702 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972671 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520920 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520925 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520933 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520935 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520936 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520938 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648796 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648811 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648812 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648774 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648782 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648783 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454036 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454043 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454057 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454068 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454078 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454082 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888539 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888556 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888566 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888578 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888579 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888589 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986460 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986466 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986467 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986463 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986461 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986465 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986462 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074566 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074571 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074572 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074582 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074585 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074587 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571202 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571192 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571194 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571197 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571199 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178178 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178175 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178176 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178177 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178491 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212726 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212727 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212728 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212729 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212730 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071824 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071825 at page 9/9 for 720h window\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guard] stop sensor 13071826 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071827 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071828 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193873 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193874 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193875 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193876 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193877 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193868 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193869 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193870 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193871 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193872 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322774 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322775 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322776 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322777 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322778 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594426 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594427 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594428 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594429 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594430 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719588 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719589 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719590 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719591 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719592 at page 9/9 for 720h window\n",
      "                      ts_utc parameter  value   unit        lat        lon  \\\n",
      "0  2016-03-06 19:00:00+00:00       no2  0.007    ppm  40.670250 -74.126083   \n",
      "1  2016-03-06 19:00:00+00:00        o3  0.038    ppm  40.670250 -74.126083   \n",
      "2  2016-03-06 19:00:00+00:00       so2  0.000    ppm  40.670250 -74.126083   \n",
      "3  2016-03-06 19:00:00+00:00        co  0.200    ppm  40.757500 -74.200500   \n",
      "4  2016-03-06 19:00:00+00:00        co  0.400    ppm  40.662388 -74.214813   \n",
      "5  2016-03-06 19:00:00+00:00       so2  0.000    ppm  40.662388 -74.214813   \n",
      "6  2016-03-06 19:00:00+00:00        co  0.100    ppm  40.641441 -74.208366   \n",
      "7  2016-03-06 19:00:00+00:00       no2  0.003    ppm  40.641441 -74.208366   \n",
      "8  2016-03-06 19:00:00+00:00      pm25  8.200  µg/m³  40.641441 -74.208366   \n",
      "9  2016-03-06 19:00:00+00:00       so2  0.000    ppm  40.641441 -74.208366   \n",
      "10 2016-03-06 19:00:00+00:00        co  0.100    ppm  40.853550 -73.966100   \n",
      "11 2016-03-06 19:00:00+00:00       no2  0.005    ppm  40.853550 -73.966100   \n",
      "\n",
      "    sensor_id  locationId            location  city country  \n",
      "0        2644        1496             Bayonne  <NA>    <NA>  \n",
      "1        2645        1496             Bayonne  <NA>    <NA>  \n",
      "2        2646        1496             Bayonne  <NA>    <NA>  \n",
      "3        1522         853         East Orange  <NA>    <NA>  \n",
      "4        3954        2193           Elizabeth  <NA>    <NA>  \n",
      "5        3953        2193           Elizabeth  <NA>    <NA>  \n",
      "6        3952         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "7        3951         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "8        1758         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "9        3950         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "10       1536         857  Fort Lee Near Road  <NA>    <NA>  \n",
      "11       1535         857  Fort Lee Near Road  <NA>    <NA>  \n",
      "[i] rows fetched: 159992\n",
      "[✓] wrote 159992 rows → openaq_new_york_bbox_last_month_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd, time, math, random\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from math import ceil\n",
    "\n",
    "# =========== SETTINGS ===========\n",
    "DEBUG = True\n",
    "CITY = \"New York\"\n",
    "CITY_COORDS = {\n",
    "    \"San Francisco\": (37.7749, -122.4194),\n",
    "    \"Los Angeles\":   (34.0522, -118.2437),\n",
    "    \"New York\":      (40.7128, -74.0060),\n",
    "    \"Chicago\":       (41.8781, -87.6298),\n",
    "    \"Houston\":       (29.7604, -95.3698),\n",
    "    \"Toronto\":       (43.6532, -79.3832),\n",
    "    \"Vancouver\":     (49.2827, -123.1207),\n",
    "}\n",
    "CENTER_LAT, CENTER_LON = CITY_COORDS[CITY]\n",
    "\n",
    "RADIUS_M = 25000\n",
    "PER_PAGE  = 100          # OpenAQ v3 max=100\n",
    "SLEEP     = 0.15\n",
    "LOC_MAX   = 200\n",
    "\n",
    "# ⬇️ Keep ALL pollutants; or e.g. [\"pm25\",\"pm10\",\"no2\",\"o3\",\"so2\",\"co\"]\n",
    "POLLUTANTS = None\n",
    "\n",
    "# Last month window\n",
    "NOW_UTC   = datetime.now(timezone.utc)\n",
    "DATE_TO   = NOW_UTC\n",
    "DATE_FROM = NOW_UTC - timedelta(days=30)\n",
    "\n",
    "OUT_CSV   = f\"openaq_{CITY.replace(' ','_').lower()}_bbox_last_month_hourly.csv\"\n",
    "\n",
    "API_KEY = \"e78f079d1ae5390a89f428671ff1349bdefdee203a74f0aff047cee62474901d\"\n",
    "BASE   = \"https://api.openaq.org/v3\"\n",
    "HDRS   = {\"Accept\": \"application/json\", \"X-API-Key\": API_KEY}\n",
    "\n",
    "# --- Session: disable status-based auto-retries; we handle them ourselves\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=0,                 # no automatic status retries\n",
    "        connect=6, read=6,       # keep transient network retries\n",
    "        backoff_factor=0.0,\n",
    "        respect_retry_after_header=True,\n",
    "        status_forcelist=[],     # empty: we manage 408/429/5xx manually\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "# --- Our backoff: handles 408/429/500/502/503/504 with Retry-After + jitter\n",
    "def get_with_backoff(url, params=None, timeout=60, max_attempts=7):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        r = SESSION.get(url, headers=HDRS, params=params, timeout=timeout)\n",
    "\n",
    "        if r.status_code in (408, 429, 500, 502, 503, 504) and attempt < max_attempts:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                sleep_s = float(ra) if ra is not None else min(30.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            except Exception:\n",
    "                sleep_s = min(30.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            if DEBUG:\n",
    "                print(f\"[backoff] {r.status_code} {url} — sleeping {sleep_s:.1f}s (attempt {attempt}/{max_attempts})\")\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "def bbox_from_center(lat, lon, radius_m):\n",
    "    km = radius_m / 1000.0\n",
    "    dlat = km / 111.0\n",
    "    dlon = km / (111.0 * max(0.1, math.cos(math.radians(lat))))\n",
    "    xmin, ymin = lon - dlon, lat - dlat\n",
    "    xmax, ymax = lon + dlon, lat + dlat\n",
    "    return f\"{xmin},{ymin},{xmax},{ymax}\"\n",
    "\n",
    "def norm_param(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, dict):\n",
    "        return str(x.get(\"name\",\"\")).lower() or None\n",
    "    return str(x).lower()\n",
    "\n",
    "# ---------- Helpers to discover locations & sensors in bbox ----------\n",
    "def list_locations_bbox(lat, lon, radius_m, per_page=PER_PAGE, limit_locations=LOC_MAX):\n",
    "    url = f\"{BASE}/locations\"\n",
    "    bbox = bbox_from_center(lat, lon, radius_m)\n",
    "    rows, page = [], 1\n",
    "    while len(rows) < limit_locations:\n",
    "        params = {\"bbox\": bbox, \"limit\": per_page, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if DEBUG and page == 1:\n",
    "            print(f\"[locations] page1 got {len(res)} rows\")\n",
    "        if len(res) < per_page: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    df = pd.json_normalize(rows)\n",
    "    keep = [\"id\",\"name\",\"coordinates.latitude\",\"coordinates.longitude\",\"city\",\"country\",\"timezone\"]\n",
    "    for c in keep:\n",
    "        if c not in df.columns: df[c] = pd.NA\n",
    "    df = df.rename(columns={\n",
    "        \"id\":\"locations_id\",\n",
    "        \"coordinates.latitude\":\"lat\",\n",
    "        \"coordinates.longitude\":\"lon\",\n",
    "    })\n",
    "    df = df.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "    df = df[[\"locations_id\",\"name\",\"lat\",\"lon\",\"city\",\"country\",\"timezone\"]].head(limit_locations)\n",
    "    return df\n",
    "\n",
    "def list_sensors_for_location(loc_id):\n",
    "    url = f\"{BASE}/locations/{int(loc_id)}/sensors\"\n",
    "    rows, page = [], 1\n",
    "    while True:\n",
    "        params = {\"limit\": PER_PAGE, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if len(res) < PER_PAGE: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "# ---------- Fetch last month of HOURLY data per sensor (robust) ----------\n",
    "def fetch_last_month_hourly_for_bbox(center_lat, center_lon, radius_m, date_from, date_to, per_page=PER_PAGE):\n",
    "    locs = list_locations_bbox(center_lat, center_lon, radius_m)\n",
    "    if locs.empty:\n",
    "        print(\"[i] No locations in bbox.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    pollutants_set = set([p.lower() for p in POLLUTANTS]) if POLLUTANTS else None\n",
    "    all_rows = []\n",
    "\n",
    "    # pages needed for the window (hourly, per_page rows per page)\n",
    "    hours_span = max(1, int((date_to - date_from).total_seconds() // 3600))\n",
    "    max_pages_needed = ceil(hours_span / per_page) + 1  # +1 slack\n",
    "\n",
    "    for _, loc in locs.iterrows():\n",
    "        sensors = list_sensors_for_location(loc[\"locations_id\"])\n",
    "        if sensors.empty:\n",
    "            continue\n",
    "\n",
    "        for _, s in sensors.iterrows():\n",
    "            sensor_id = s.get(\"id\")\n",
    "            if pd.isna(sensor_id):\n",
    "                continue\n",
    "\n",
    "            # Early skip by pollutant at sensor level (if available)\n",
    "            if pollutants_set:\n",
    "                sn = (s.get(\"parameter.name\") or s.get(\"parameter\") or \"\")\n",
    "                if isinstance(sn, dict):  # sometimes parameter is dict\n",
    "                    sn = sn.get(\"name\", \"\")\n",
    "                if (sn or \"\").lower() not in pollutants_set:\n",
    "                    continue\n",
    "\n",
    "            page = 1\n",
    "            while True:\n",
    "                url = f\"{BASE}/sensors/{int(sensor_id)}/hours\"\n",
    "                params = {\n",
    "                    \"limit\": per_page,\n",
    "                    \"page\": page,\n",
    "                    \"date_from\": date_from.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                    \"date_to\":   date_to.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                    \"sort\": \"asc\",  # oldest→newest\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    r = get_with_backoff(url, params=params, timeout=60)\n",
    "                except requests.HTTPError as e:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[warn] sensor {sensor_id} page {page} HTTPError: {e}\")\n",
    "                    break\n",
    "                except requests.RequestException as e:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[warn] sensor {sensor_id} page {page} RequestException: {e}\")\n",
    "                    break\n",
    "\n",
    "                res = r.json().get(\"results\", []) or []\n",
    "                if not res:\n",
    "                    break\n",
    "\n",
    "                for rec in res:\n",
    "                    value = pd.to_numeric(rec.get(\"value\"), errors=\"coerce\")\n",
    "                    param_obj = rec.get(\"parameter\") or {}\n",
    "                    parameter = (param_obj.get(\"name\") or \"\").lower() or None\n",
    "                    unit = param_obj.get(\"units\")\n",
    "\n",
    "                    if pollutants_set and (parameter not in pollutants_set):\n",
    "                        continue\n",
    "\n",
    "                    period = rec.get(\"period\") or {}\n",
    "                    dt_from = (period.get(\"datetimeFrom\") or {}).get(\"utc\")\n",
    "                    ts_utc = pd.to_datetime(dt_from, errors=\"coerce\", utc=True) if isinstance(dt_from, str) else pd.NaT\n",
    "\n",
    "                    # coords: prefer sensor latest coords, else fallback to location coords\n",
    "                    lat = s.get(\"latest.coordinates.latitude\", loc[\"lat\"])\n",
    "                    lon = s.get(\"latest.coordinates.longitude\", loc[\"lon\"])\n",
    "\n",
    "                    all_rows.append({\n",
    "                        \"ts_utc\": ts_utc,\n",
    "                        \"parameter\": parameter,\n",
    "                        \"value\": value,\n",
    "                        \"unit\": unit,\n",
    "                        \"lat\": pd.to_numeric(lat, errors=\"coerce\"),\n",
    "                        \"lon\": pd.to_numeric(lon, errors=\"coerce\"),\n",
    "                        \"sensor_id\": sensor_id,\n",
    "                        \"locationId\": int(loc[\"locations_id\"]),\n",
    "                        \"location\": loc[\"name\"],\n",
    "                        \"city\": loc.get(\"city\"),\n",
    "                        \"country\": loc.get(\"country\"),\n",
    "                    })\n",
    "\n",
    "                # pagination guards\n",
    "                if len(res) < per_page:\n",
    "                    break\n",
    "                page += 1\n",
    "                if page > max_pages_needed:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[guard] stop sensor {sensor_id} at page {page-1}/{max_pages_needed} for {hours_span}h window\")\n",
    "                    break\n",
    "                time.sleep(SLEEP)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "    if df[\"ts_utc\"].notna().any():\n",
    "        df = df.sort_values([\"ts_utc\",\"location\",\"parameter\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    print(f\"CITY: {CITY} @ ({CENTER_LAT}, {CENTER_LON})\")\n",
    "    print(f\"Window (UTC): {DATE_FROM.strftime('%Y-%m-%d %H:%M:%S')}  →  {DATE_TO.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    df = fetch_last_month_hourly_for_bbox(CENTER_LAT, CENTER_LON, RADIUS_M, DATE_FROM, DATE_TO)\n",
    "    if df.empty:\n",
    "        print(\"[i] No hourly values found. Try increasing RADIUS_M or widening dates.\")\n",
    "        return\n",
    "\n",
    "    if DEBUG:\n",
    "        print(df.head(12))\n",
    "        print(f\"[i] rows fetched: {len(df)}\")\n",
    "\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[✓] wrote {len(df)} rows → {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5832becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITY: New York @ (40.7128, -74.006)\n",
      "Window (UTC): 2025-09-04 12:38:06  →  2025-10-04 12:38:06\n",
      "[locations] page1 got 57 rows\n",
      "[guard] stop sensor 673 at page 9/9 for 720h window\n",
      "[guard] stop sensor 671 at page 9/9 for 720h window\n",
      "[guard] stop sensor 674 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1097 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1102 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1099 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1098 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1103 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1152 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1106 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1121 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1128 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1143 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1145 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1146 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1147 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1522 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1523 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1534 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1535 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1536 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272431 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272250 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1693 at page 9/9 for 720h window\n",
      "[guard] stop sensor 5077566 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1758 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3950 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3951 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3952 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272273 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272290 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3639 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3638 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3640 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1761 at page 9/9 for 720h window\n",
      "[guard] stop sensor 3637 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1777 at page 9/9 for 720h window\n",
      "[guard] stop sensor 23341 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2016 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2018 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272080 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272352 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2646 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2645 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2644 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272224 at page 9/9 for 720h window\n",
      "[guard] stop sensor 4272086 at page 9/9 for 720h window\n",
      "[warn] sensor 3954 page 9 RequestException: HTTPSConnectionPool(host='api.openaq.org', port=443): Read timed out.\n",
      "[guard] stop sensor 3953 at page 9/9 for 720h window\n",
      "[guard] stop sensor 25520 at page 9/9 for 720h window\n",
      "[guard] stop sensor 1662910 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2165515 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2189530 at page 9/9 for 720h window\n",
      "[guard] stop sensor 2780575 at page 9/9 for 720h window\n",
      "[guard] stop sensor 6530280 at page 9/9 for 720h window\n",
      "[guard] stop sensor 6530283 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970982 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971373 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972608 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7324143 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972609 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7324145 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971073 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971082 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972670 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972707 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7386938 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7386942 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970916 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970930 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7762999 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7763001 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971870 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971901 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971145 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971292 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971629 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971668 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7815388 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7815389 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971262 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971316 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972696 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972703 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897332 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897339 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7970808 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7971247 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897354 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7897355 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972702 at page 9/9 for 720h window\n",
      "[guard] stop sensor 7972671 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520920 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520925 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520933 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520935 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520936 at page 9/9 for 720h window\n",
      "[guard] stop sensor 8520938 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648796 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648811 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648812 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648774 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648782 at page 9/9 for 720h window\n",
      "[guard] stop sensor 9648783 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454036 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454043 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454057 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454068 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454078 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10454082 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888539 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888556 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888566 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888578 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888579 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10888589 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986460 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986466 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986467 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986463 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986461 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986465 at page 9/9 for 720h window\n",
      "[guard] stop sensor 10986462 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074566 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074571 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074572 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074582 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074585 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11074587 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571202 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571192 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571194 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571197 at page 9/9 for 720h window\n",
      "[guard] stop sensor 11571199 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178178 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178175 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178176 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178177 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12178491 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212726 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212727 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212728 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212729 at page 9/9 for 720h window\n",
      "[guard] stop sensor 12212730 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071824 at page 9/9 for 720h window\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guard] stop sensor 13071825 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071826 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071827 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13071828 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193873 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193874 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193875 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193876 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193877 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193868 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193869 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193870 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193871 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13193872 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322774 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322775 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322776 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322777 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13322778 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594426 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594427 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594428 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594429 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13594430 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719588 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719589 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719590 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719591 at page 9/9 for 720h window\n",
      "[guard] stop sensor 13719592 at page 9/9 for 720h window\n",
      "                      ts_utc parameter  value   unit        lat        lon  \\\n",
      "0  2016-03-06 19:00:00+00:00       no2  0.007    ppm  40.670250 -74.126083   \n",
      "1  2016-03-06 19:00:00+00:00        o3  0.038    ppm  40.670250 -74.126083   \n",
      "2  2016-03-06 19:00:00+00:00       so2  0.000    ppm  40.670250 -74.126083   \n",
      "3  2016-03-06 19:00:00+00:00        co  0.200    ppm  40.757500 -74.200500   \n",
      "4  2016-03-06 19:00:00+00:00        co  0.400    ppm  40.662388 -74.214813   \n",
      "5  2016-03-06 19:00:00+00:00       so2  0.000    ppm  40.662388 -74.214813   \n",
      "6  2016-03-06 19:00:00+00:00        co  0.100    ppm  40.641441 -74.208366   \n",
      "7  2016-03-06 19:00:00+00:00       no2  0.003    ppm  40.641441 -74.208366   \n",
      "8  2016-03-06 19:00:00+00:00      pm25  8.200  µg/m³  40.641441 -74.208366   \n",
      "9  2016-03-06 19:00:00+00:00       so2  0.000    ppm  40.641441 -74.208366   \n",
      "10 2016-03-06 19:00:00+00:00        co  0.100    ppm  40.853550 -73.966100   \n",
      "11 2016-03-06 19:00:00+00:00       no2  0.005    ppm  40.853550 -73.966100   \n",
      "\n",
      "    sensor_id  locationId            location  city country  \n",
      "0        2644        1496             Bayonne  <NA>    <NA>  \n",
      "1        2645        1496             Bayonne  <NA>    <NA>  \n",
      "2        2646        1496             Bayonne  <NA>    <NA>  \n",
      "3        1522         853         East Orange  <NA>    <NA>  \n",
      "4        3954        2193           Elizabeth  <NA>    <NA>  \n",
      "5        3953        2193           Elizabeth  <NA>    <NA>  \n",
      "6        3952         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "7        3951         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "8        1758         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "9        3950         971   Elizabeth Trailer  <NA>    <NA>  \n",
      "10       1536         857  Fort Lee Near Road  <NA>    <NA>  \n",
      "11       1535         857  Fort Lee Near Road  <NA>    <NA>  \n",
      "[i] rows fetched (long): 159892\n",
      "[✓] wrote long → openaq_new_york_bbox_last_month_hourly_long.csv\n",
      "[✓] wide shape: (58127, 31)\n",
      "                     ts_utc       lat       lon  locationId  \\\n",
      "0 2016-03-06 19:00:00+00:00  40.60394 -74.27618         924   \n",
      "1 2016-03-06 19:00:00+00:00  40.64144 -74.20837         971   \n",
      "2 2016-03-06 19:00:00+00:00  40.66239 -74.21481        2193   \n",
      "3 2016-03-06 19:00:00+00:00  40.67025 -74.12608        1496   \n",
      "4 2016-03-06 19:00:00+00:00  40.72099 -74.19289         974   \n",
      "5 2016-03-06 19:00:00+00:00  40.73169 -74.06657        1122   \n",
      "6 2016-03-06 19:00:00+00:00  40.75750 -74.20050         853   \n",
      "7 2016-03-06 19:00:00+00:00  40.85355 -73.96610         857   \n",
      "8 2016-03-06 20:00:00+00:00  40.60394 -74.27618         924   \n",
      "9 2016-03-06 20:00:00+00:00  40.64144 -74.20837         971   \n",
      "\n",
      "             location  city country   co  no    no2  ...  no_unit  nox_unit  \\\n",
      "0           Rahway PM  <NA>    <NA>  NaN NaN    NaN  ...      NaN       NaN   \n",
      "1   Elizabeth Trailer  <NA>    <NA>  0.1 NaN  0.003  ...      NaN       NaN   \n",
      "2           Elizabeth  <NA>    <NA>  0.4 NaN    NaN  ...      NaN       NaN   \n",
      "3             Bayonne  <NA>    <NA>  NaN NaN  0.007  ...      NaN       NaN   \n",
      "4    Newark Firehouse  <NA>    <NA>  0.2 NaN  0.003  ...      NaN       NaN   \n",
      "5         Jersey City  <NA>    <NA>  0.3 NaN    NaN  ...      NaN       NaN   \n",
      "6         East Orange  <NA>    <NA>  0.2 NaN    NaN  ...      NaN       NaN   \n",
      "7  Fort Lee Near Road  <NA>    <NA>  0.1 NaN  0.005  ...      NaN       NaN   \n",
      "8           Rahway PM  <NA>    <NA>  NaN NaN    NaN  ...      NaN       NaN   \n",
      "9   Elizabeth Trailer  <NA>    <NA>  0.1 NaN  0.003  ...      NaN       NaN   \n",
      "\n",
      "   o3_unit  pm10_unit  pm1_unit  pm25_unit  relativehumidity_unit  so2_unit  \\\n",
      "0      NaN        NaN       NaN      µg/m³                    NaN       NaN   \n",
      "1      NaN        NaN       NaN      µg/m³                    NaN       ppm   \n",
      "2      NaN        NaN       NaN        NaN                    NaN       ppm   \n",
      "3      ppm        NaN       NaN        NaN                    NaN       ppm   \n",
      "4      ppm        NaN       NaN      µg/m³                    NaN       ppm   \n",
      "5      NaN        NaN       NaN        NaN                    NaN       ppm   \n",
      "6      NaN        NaN       NaN        NaN                    NaN       NaN   \n",
      "7      NaN        NaN       NaN      µg/m³                    NaN       NaN   \n",
      "8      NaN        NaN       NaN      µg/m³                    NaN       NaN   \n",
      "9      NaN        NaN       NaN      µg/m³                    NaN       ppm   \n",
      "\n",
      "   temperature_unit um003_unit  \n",
      "0               NaN        NaN  \n",
      "1               NaN        NaN  \n",
      "2               NaN        NaN  \n",
      "3               NaN        NaN  \n",
      "4               NaN        NaN  \n",
      "5               NaN        NaN  \n",
      "6               NaN        NaN  \n",
      "7               NaN        NaN  \n",
      "8               NaN        NaN  \n",
      "9               NaN        NaN  \n",
      "\n",
      "[10 rows x 31 columns]\n",
      "[✓] wrote wide → openaq_new_york_bbox_last_month_hourly_wide.csv\n"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd, time, math, random\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from math import ceil\n",
    "\n",
    "# =========== SETTINGS ===========\n",
    "DEBUG = True\n",
    "CITY = \"New York\"\n",
    "CITY_COORDS = {\n",
    "    \"San Francisco\": (37.7749, -122.4194),\n",
    "    \"Los Angeles\":   (34.0522, -118.2437),\n",
    "    \"New York\":      (40.7128, -74.0060),\n",
    "    \"Chicago\":       (41.8781, -87.6298),\n",
    "    \"Houston\":       (29.7604, -95.3698),\n",
    "    \"Toronto\":       (43.6532, -79.3832),\n",
    "    \"Vancouver\":     (49.2827, -123.1207),\n",
    "}\n",
    "CENTER_LAT, CENTER_LON = CITY_COORDS[CITY]\n",
    "\n",
    "RADIUS_M = 25000\n",
    "PER_PAGE  = 100          # OpenAQ v3 max=100\n",
    "SLEEP     = 0.15\n",
    "LOC_MAX   = 200\n",
    "\n",
    "# ⬇️ Keep ALL pollutants; or e.g. [\"pm25\",\"pm10\",\"no2\",\"o3\",\"so2\",\"co\"]\n",
    "POLLUTANTS = None\n",
    "\n",
    "# Last month window\n",
    "NOW_UTC   = datetime.now(timezone.utc)\n",
    "DATE_TO   = NOW_UTC\n",
    "DATE_FROM = NOW_UTC - timedelta(days=30)\n",
    "\n",
    "# Outputs\n",
    "OUT_CSV_LONG = f\"openaq_{CITY.replace(' ','_').lower()}_bbox_last_month_hourly_long.csv\"   # hour x location x parameter\n",
    "OUT_CSV_WIDE = f\"openaq_{CITY.replace(' ','_').lower()}_bbox_last_month_hourly_wide.csv\"   # hour x point, columns per parameter\n",
    "COORD_DECIMALS = 5\n",
    "INCLUDE_UNIT_COLS = True\n",
    "\n",
    "API_KEY = \"e78f079d1ae5390a89f428671ff1349bdefdee203a74f0aff047cee62474901d\"\n",
    "BASE   = \"https://api.openaq.org/v3\"\n",
    "HDRS   = {\"Accept\": \"application/json\", \"X-API-Key\": API_KEY}\n",
    "\n",
    "# --- Session: disable status-based auto-retries; we handle them ourselves\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=0,                 # no automatic status retries\n",
    "        connect=6, read=6,       # keep transient network retries\n",
    "        backoff_factor=0.0,\n",
    "        respect_retry_after_header=True,\n",
    "        status_forcelist=[],     # empty: we manage 408/429/5xx manually\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "# --- Our backoff: handles 408/429/500/502/503/504 with Retry-After + jitter\n",
    "def get_with_backoff(url, params=None, timeout=60, max_attempts=7):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        r = SESSION.get(url, headers=HDRS, params=params, timeout=timeout)\n",
    "\n",
    "        if r.status_code in (408, 429, 500, 502, 503, 504) and attempt < max_attempts:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                sleep_s = float(ra) if ra is not None else min(30.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            except Exception:\n",
    "                sleep_s = min(30.0, (2 ** attempt) + random.uniform(0, 1.0))\n",
    "            if DEBUG:\n",
    "                print(f\"[backoff] {r.status_code} {url} — sleeping {sleep_s:.1f}s (attempt {attempt}/{max_attempts})\")\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "def bbox_from_center(lat, lon, radius_m):\n",
    "    km = radius_m / 1000.0\n",
    "    dlat = km / 111.0\n",
    "    dlon = km / (111.0 * max(0.1, math.cos(math.radians(lat))))\n",
    "    xmin, ymin = lon - dlon, lat - dlat\n",
    "    xmax, ymax = lon + dlon, lat + dlat\n",
    "    return f\"{xmin},{ymin},{xmax},{ymax}\"\n",
    "\n",
    "def norm_param(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, dict):\n",
    "        return str(x.get(\"name\",\"\")).lower() or None\n",
    "    return str(x).lower()\n",
    "\n",
    "# ---------- Helpers to discover locations & sensors in bbox ----------\n",
    "def list_locations_bbox(lat, lon, radius_m, per_page=PER_PAGE, limit_locations=LOC_MAX):\n",
    "    url = f\"{BASE}/locations\"\n",
    "    bbox = bbox_from_center(lat, lon, radius_m)\n",
    "    rows, page = [], 1\n",
    "    while len(rows) < limit_locations:\n",
    "        params = {\"bbox\": bbox, \"limit\": per_page, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if DEBUG and page == 1:\n",
    "            print(f\"[locations] page1 got {len(res)} rows\")\n",
    "        if len(res) < per_page: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    df = pd.json_normalize(rows)\n",
    "    keep = [\"id\",\"name\",\"coordinates.latitude\",\"coordinates.longitude\",\"city\",\"country\",\"timezone\"]\n",
    "    for c in keep:\n",
    "        if c not in df.columns: df[c] = pd.NA\n",
    "    df = df.rename(columns={\n",
    "        \"id\":\"locations_id\",\n",
    "        \"coordinates.latitude\":\"lat\",\n",
    "        \"coordinates.longitude\":\"lon\",\n",
    "    })\n",
    "    df = df.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "    df = df[[\"locations_id\",\"name\",\"lat\",\"lon\",\"city\",\"country\",\"timezone\"]].head(limit_locations)\n",
    "    return df\n",
    "\n",
    "def list_sensors_for_location(loc_id):\n",
    "    url = f\"{BASE}/locations/{int(loc_id)}/sensors\"\n",
    "    rows, page = [], 1\n",
    "    while True:\n",
    "        params = {\"limit\": PER_PAGE, \"page\": page}\n",
    "        r = get_with_backoff(url, params=params, timeout=30)\n",
    "        res = r.json().get(\"results\", []) or []\n",
    "        rows.extend(res)\n",
    "        if len(res) < PER_PAGE: break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "# ---------- Fetch last month of HOURLY data per sensor (robust) ----------\n",
    "def fetch_last_month_hourly_for_bbox(center_lat, center_lon, radius_m, date_from, date_to, per_page=PER_PAGE):\n",
    "    locs = list_locations_bbox(center_lat, center_lon, radius_m)\n",
    "    if locs.empty:\n",
    "        print(\"[i] No locations in bbox.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    pollutants_set = set([p.lower() for p in POLLUTANTS]) if POLLUTANTS else None\n",
    "    all_rows = []\n",
    "\n",
    "    # pages needed for the window (hourly, per_page rows per page)\n",
    "    hours_span = max(1, int((date_to - date_from).total_seconds() // 3600))\n",
    "    max_pages_needed = ceil(hours_span / per_page) + 1  # +1 slack\n",
    "\n",
    "    for _, loc in locs.iterrows():\n",
    "        sensors = list_sensors_for_location(loc[\"locations_id\"])\n",
    "        if sensors.empty:\n",
    "            continue\n",
    "\n",
    "        for _, s in sensors.iterrows():\n",
    "            sensor_id = s.get(\"id\")\n",
    "            if pd.isna(sensor_id):\n",
    "                continue\n",
    "\n",
    "            # Early skip by pollutant at sensor level (if available)\n",
    "            if pollutants_set:\n",
    "                sn = (s.get(\"parameter.name\") or s.get(\"parameter\") or \"\")\n",
    "                if isinstance(sn, dict):\n",
    "                    sn = sn.get(\"name\", \"\")\n",
    "                if (sn or \"\").lower() not in pollutants_set:\n",
    "                    continue\n",
    "\n",
    "            page = 1\n",
    "            while True:\n",
    "                url = f\"{BASE}/sensors/{int(sensor_id)}/hours\"\n",
    "                params = {\n",
    "                    \"limit\": per_page,\n",
    "                    \"page\": page,\n",
    "                    \"date_from\": date_from.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                    \"date_to\":   date_to.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                    \"sort\": \"asc\",  # oldest→newest\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    r = get_with_backoff(url, params=params, timeout=60)\n",
    "                except requests.HTTPError as e:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[warn] sensor {sensor_id} page {page} HTTPError: {e}\")\n",
    "                    break\n",
    "                except requests.RequestException as e:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[warn] sensor {sensor_id} page {page} RequestException: {e}\")\n",
    "                    break\n",
    "\n",
    "                res = r.json().get(\"results\", []) or []\n",
    "                if not res:\n",
    "                    break\n",
    "\n",
    "                for rec in res:\n",
    "                    value = pd.to_numeric(rec.get(\"value\"), errors=\"coerce\")\n",
    "                    param_obj = rec.get(\"parameter\") or {}\n",
    "                    parameter = (param_obj.get(\"name\") or \"\").lower() or None\n",
    "                    unit = param_obj.get(\"units\")\n",
    "\n",
    "                    if pollutants_set and (parameter not in pollutants_set):\n",
    "                        continue\n",
    "\n",
    "                    period = rec.get(\"period\") or {}\n",
    "                    dt_from = (period.get(\"datetimeFrom\") or {}).get(\"utc\")\n",
    "                    ts_utc = pd.to_datetime(dt_from, errors=\"coerce\", utc=True) if isinstance(dt_from, str) else pd.NaT\n",
    "\n",
    "                    # coords: prefer sensor latest coords, else fallback to location coords\n",
    "                    lat = s.get(\"latest.coordinates.latitude\", loc[\"lat\"])\n",
    "                    lon = s.get(\"latest.coordinates.longitude\", loc[\"lon\"])\n",
    "\n",
    "                    all_rows.append({\n",
    "                        \"ts_utc\": ts_utc,\n",
    "                        \"parameter\": parameter,\n",
    "                        \"value\": value,\n",
    "                        \"unit\": unit,\n",
    "                        \"lat\": pd.to_numeric(lat, errors=\"coerce\"),\n",
    "                        \"lon\": pd.to_numeric(lon, errors=\"coerce\"),\n",
    "                        \"sensor_id\": sensor_id,\n",
    "                        \"locationId\": int(loc[\"locations_id\"]),\n",
    "                        \"location\": loc[\"name\"],\n",
    "                        \"city\": loc.get(\"city\"),\n",
    "                        \"country\": loc.get(\"country\"),\n",
    "                    })\n",
    "\n",
    "                # pagination guards\n",
    "                if len(res) < per_page:\n",
    "                    break\n",
    "                page += 1\n",
    "                if page > max_pages_needed:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[guard] stop sensor {sensor_id} at page {page-1}/{max_pages_needed} for {hours_span}h window\")\n",
    "                    break\n",
    "                time.sleep(SLEEP)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "    if df[\"ts_utc\"].notna().any():\n",
    "        df = df.sort_values([\"ts_utc\",\"location\",\"parameter\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ---- Pivot to one row per (hour, rounded lat, rounded lon) with parameter columns\n",
    "def to_wide_by_point_hourly(df: pd.DataFrame, decimals: int = COORD_DECIMALS, include_units: bool = INCLUDE_UNIT_COLS) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"ts_hour\"] = pd.to_datetime(d[\"ts_utc\"], utc=True).dt.floor(\"H\")\n",
    "    d[\"lat_r\"] = d[\"lat\"].round(decimals)\n",
    "    d[\"lon_r\"] = d[\"lon\"].round(decimals)\n",
    "\n",
    "    # If multiple sensors report the same parameter at the same site/hour → average their values\n",
    "    agg = (d.groupby([\"ts_hour\",\"lat_r\",\"lon_r\",\"parameter\",\"unit\"], dropna=False)[\"value\"]\n",
    "             .mean()\n",
    "             .reset_index())\n",
    "\n",
    "    # Pivot values to columns per parameter\n",
    "    wide_val = agg.pivot_table(index=[\"ts_hour\",\"lat_r\",\"lon_r\"], columns=\"parameter\", values=\"value\", aggfunc=\"first\")\n",
    "\n",
    "    if include_units:\n",
    "        # one unit per parameter per hour/point (just take the unit string)\n",
    "        agg[\"param_unit\"] = agg[\"parameter\"] + \"_unit\"\n",
    "        wide_unit = agg.pivot_table(index=[\"ts_hour\",\"lat_r\",\"lon_r\"], columns=\"param_unit\", values=\"unit\", aggfunc=\"first\")\n",
    "        wide = pd.concat([wide_val, wide_unit], axis=1)\n",
    "    else:\n",
    "        wide = wide_val\n",
    "\n",
    "    # Attach representative metadata (locationId/location/city/country) using most frequent at that point/hour\n",
    "    meta_cols = [c for c in [\"locationId\",\"location\",\"city\",\"country\"] if c in d.columns]\n",
    "    for mc in meta_cols:\n",
    "        meta = (d.groupby([\"ts_hour\",\"lat_r\",\"lon_r\"])[mc]\n",
    "                  .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[-1]))\n",
    "        wide = wide.join(meta, on=[\"ts_hour\",\"lat_r\",\"lon_r\"])\n",
    "\n",
    "    wide = wide.reset_index()\n",
    "    wide = wide.rename(columns={\"ts_hour\":\"ts_utc\",\"lat_r\":\"lat\",\"lon_r\":\"lon\"})\n",
    "    # reorder\n",
    "    param_cols = sorted([c for c in wide.columns if c not in {\"ts_utc\",\"lat\",\"lon\",\"locationId\",\"location\",\"city\",\"country\"} and not str(c).endswith(\"_unit\")])\n",
    "    unit_cols  = sorted([c for c in wide.columns if str(c).endswith(\"_unit\")])\n",
    "    ordered = ([\"ts_utc\",\"lat\",\"lon\",\"locationId\",\"location\",\"city\",\"country\"] + param_cols + unit_cols)\n",
    "    wide = wide[ordered]\n",
    "    return wide\n",
    "\n",
    "def main():\n",
    "    print(f\"CITY: {CITY} @ ({CENTER_LAT}, {CENTER_LON})\")\n",
    "    print(f\"Window (UTC): {DATE_FROM.strftime('%Y-%m-%d %H:%M:%S')}  →  {DATE_TO.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    df = fetch_last_month_hourly_for_bbox(CENTER_LAT, CENTER_LON, RADIUS_M, DATE_FROM, DATE_TO)\n",
    "    if df.empty:\n",
    "        print(\"[i] No hourly values found. Try increasing RADIUS_M or widening dates.\")\n",
    "        return\n",
    "\n",
    "    if DEBUG:\n",
    "        print(df.head(12))\n",
    "        print(f\"[i] rows fetched (long): {len(df)}\")\n",
    "\n",
    "    # LONG: one line per (hour, location, parameter)\n",
    "    df_long = (df.assign(ts_hour=lambda d: pd.to_datetime(d[\"ts_utc\"], utc=True).dt.floor(\"H\"))\n",
    "                 .sort_values([\"ts_hour\",\"location\",\"parameter\"])\n",
    "                 .reset_index(drop=True))\n",
    "    df_long.to_csv(OUT_CSV_LONG, index=False)\n",
    "    print(f\"[✓] wrote long → {OUT_CSV_LONG}\")\n",
    "\n",
    "    # WIDE: one line per (hour, rounded lat/lon), columns per parameter (+ optional _unit)\n",
    "    wide = to_wide_by_point_hourly(df)\n",
    "    print(f\"[✓] wide shape: {wide.shape}\")\n",
    "    if DEBUG:\n",
    "        print(wide.head(10))\n",
    "    wide.to_csv(OUT_CSV_WIDE, index=False)\n",
    "    print(f\"[✓] wrote wide → {OUT_CSV_WIDE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06049df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18280\\1632791087.py:77: DtypeWarning: Columns (19,20,21,22,23,25,27,28,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] wrote 3814 rows → openaq_new_york_bbox_last_month_daily_aqi.csv\n",
      "   locationId                      date  pm25_conc  pm10_conc   o3_conc  \\\n",
      "0         384 2016-03-12 00:00:00+00:00   2.400000       21.0       NaN   \n",
      "1         384 2016-03-26 00:00:00+00:00   2.400000       21.0       NaN   \n",
      "2         384 2016-03-31 00:00:00+00:00   8.550000       21.0       NaN   \n",
      "3         384 2016-04-01 00:00:00+00:00  10.411765       21.0  0.023833   \n",
      "4         384 2016-04-02 00:00:00+00:00   5.020000       21.0  0.021001   \n",
      "5         384 2016-04-03 00:00:00+00:00   4.700000       21.0       NaN   \n",
      "6         384 2016-05-12 00:00:00+00:00  15.400000       21.0       NaN   \n",
      "7         384 2016-05-21 00:00:00+00:00   6.600000       21.0       NaN   \n",
      "8         384 2016-08-04 00:00:00+00:00   6.900000       21.0       NaN   \n",
      "9         384 2016-08-15 00:00:00+00:00   5.800000       21.0       NaN   \n",
      "\n",
      "    co_conc  no2_conc  so2_conc   aqi_pm25   aqi_pm10  ...    aqi_co  \\\n",
      "0       NaN  0.004783       0.0  10.000000  19.444444  ...       NaN   \n",
      "1       NaN  0.004783       0.0  10.000000  19.444444  ...       NaN   \n",
      "2       NaN  0.004783       0.0  35.625000  19.444444  ...       NaN   \n",
      "3  0.000175  0.004783       0.0  43.382353  19.444444  ...  0.001984   \n",
      "4  0.000175  0.004783       0.0  20.916667  19.444444  ...  0.001984   \n",
      "5       NaN  0.004783       0.0  19.583333  19.444444  ...       NaN   \n",
      "6       NaN  0.004783       0.0  57.939914  19.444444  ...       NaN   \n",
      "7       NaN  0.004783       0.0  27.500000  19.444444  ...       NaN   \n",
      "8       NaN  0.004783       0.0  28.750000  19.444444  ...       NaN   \n",
      "9       NaN  0.004783       0.0  24.166667  19.444444  ...       NaN   \n",
      "\n",
      "    aqi_no2  aqi_so2        aqi  aqi_next_24h  location city  country  \\\n",
      "0  0.004512      0.0  19.444444     19.444444      CCNY  NaN      NaN   \n",
      "1  0.004512      0.0  19.444444     35.625000      CCNY  NaN      NaN   \n",
      "2  0.004512      0.0  35.625000     43.382353      CCNY  NaN      NaN   \n",
      "3  0.004512      0.0  43.382353     20.916667      CCNY  NaN      NaN   \n",
      "4  0.004512      0.0  20.916667     19.583333      CCNY  NaN      NaN   \n",
      "5  0.004512      0.0  19.583333     57.939914      CCNY  NaN      NaN   \n",
      "6  0.004512      0.0  57.939914     27.500000      CCNY  NaN      NaN   \n",
      "7  0.004512      0.0  27.500000     28.750000      CCNY  NaN      NaN   \n",
      "8  0.004512      0.0  28.750000     24.166667      CCNY  NaN      NaN   \n",
      "9  0.004512      0.0  24.166667     36.875000      CCNY  NaN      NaN   \n",
      "\n",
      "       lat      lon  \n",
      "0  40.8197 -73.9481  \n",
      "1  40.8197 -73.9481  \n",
      "2  40.8197 -73.9481  \n",
      "3  40.8197 -73.9481  \n",
      "4  40.8197 -73.9481  \n",
      "5  40.8197 -73.9481  \n",
      "6  40.8197 -73.9481  \n",
      "7  40.8197 -73.9481  \n",
      "8  40.8197 -73.9481  \n",
      "9  40.8197 -73.9481  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== USER SETTINGS =====\n",
    "INPUT_CSV  = \"openaq_new_york_bbox_last_month_hourly_wide.csv\"\n",
    "OUTPUT_CSV = Path(INPUT_CSV).with_name(Path(INPUT_CSV).stem.replace(\"_hourly_wide\",\"\") + \"_daily_aqi.csv\")\n",
    "\n",
    "# pollutants we’ll handle for AQI\n",
    "POLLUTANTS = [\"pm25\",\"pm10\",\"o3\",\"co\",\"no2\",\"so2\"]\n",
    "\n",
    "# molecular weights (for µg/m³ <-> ppm/ppb at 25°C & 1 atm, 24.45 L/mol)\n",
    "MW = {\"o3\": 48.00, \"co\": 28.01, \"no2\": 46.01, \"so2\": 64.07}\n",
    "\n",
    "# EPA AQI breakpoints (conc units noted in comments)\n",
    "AQI_BP = {\n",
    "    \"pm25\": [ # µg/m³, 24-hr\n",
    "        (0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200), (150.5, 250.4, 201, 300), (250.5, 350.4, 301, 400),\n",
    "        (350.5, 500.4, 401, 500),\n",
    "    ],\n",
    "    \"pm10\": [ # µg/m³, 24-hr\n",
    "        (0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150),\n",
    "        (255, 354, 151, 200), (355, 424, 201, 300), (425, 504, 301, 400),\n",
    "        (505, 604, 401, 500),\n",
    "    ],\n",
    "    \"o3\": [   # ppm, max 8-hr\n",
    "        (0.000, 0.054, 0, 50), (0.055, 0.070, 51, 100), (0.071, 0.085, 101, 150),\n",
    "        (0.086, 0.105, 151, 200), (0.106, 0.200, 201, 300),\n",
    "    ],\n",
    "    \"co\": [   # ppm, max 8-hr\n",
    "        (0.0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150),\n",
    "        (12.5, 15.4, 151, 200), (15.5, 30.4, 201, 300), (30.5, 40.4, 301, 400),\n",
    "        (40.5, 50.4, 401, 500),\n",
    "    ],\n",
    "    \"no2\": [  # ppb, max 1-hr\n",
    "        (0, 53, 0, 50), (54, 100, 51, 100), (101, 360, 101, 150),\n",
    "        (361, 649, 151, 200), (650, 1249, 201, 300), (1250, 1649, 301, 400),\n",
    "        (1650, 2049, 401, 500),\n",
    "    ],\n",
    "    \"so2\": [  # ppb, max 1-hr\n",
    "        (0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150),\n",
    "        (186, 304, 151, 200), (305, 604, 201, 300), (605, 804, 301, 400),\n",
    "        (805, 1004, 401, 500),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- helpers ---\n",
    "def aqi_from_bp(p, conc):\n",
    "    if pd.isna(conc):\n",
    "        return np.nan\n",
    "    for (Cl, Ch, Il, Ih) in AQI_BP[p]:\n",
    "        if Cl <= conc <= Ch:\n",
    "            return (Ih - Il) / (Ch - Cl) * (conc - Cl) + Il\n",
    "    return np.nan\n",
    "\n",
    "def ugm3_to_ppm(x, species):  # µg/m³ -> ppm\n",
    "    if pd.isna(x): return np.nan\n",
    "    return (x * 24.45) / (MW[species] * 1000.0)\n",
    "\n",
    "def ugm3_to_ppb(x, species):  # µg/m³ -> ppb\n",
    "    if pd.isna(x): return np.nan\n",
    "    return (x * 24.45) / MW[species]\n",
    "\n",
    "def ensure_ts_hour(df):\n",
    "    # choose ts column and floor to hour\n",
    "    if \"ts_hour\" in df.columns:\n",
    "        df[\"ts_hour\"] = pd.to_datetime(df[\"ts_hour\"], utc=True, errors=\"coerce\").dt.floor(\"H\")\n",
    "    else:\n",
    "        ts_col = \"ts_utc\" if \"ts_utc\" in df.columns else None\n",
    "        if ts_col is None:\n",
    "            raise ValueError(\"No timestamp column found (ts_hour or ts_utc).\")\n",
    "        df[\"ts_hour\"] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\").dt.floor(\"H\")\n",
    "    return df\n",
    "\n",
    "# --- load hourly wide ---\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = ensure_ts_hour(df)\n",
    "\n",
    "# build locationId if missing\n",
    "if \"locationId\" not in df.columns:\n",
    "    if {\"lat\",\"lon\"} <= set(df.columns):\n",
    "        df[\"locationId\"] = (df[\"lat\"].round(5).astype(str) + \",\" + df[\"lon\"].round(5).astype(str))\n",
    "    else:\n",
    "        df[\"locationId\"] = \"unknown\"\n",
    "\n",
    "# make sure pollutant columns exist (if not, create empty)\n",
    "for p in POLLUTANTS:\n",
    "    if p not in df.columns:\n",
    "        df[p] = np.nan\n",
    "\n",
    "# --- impute hourly values per locationId (ffill/bfill -> group median -> global median) ---\n",
    "df = df.sort_values([\"locationId\",\"ts_hour\"])\n",
    "for p in POLLUTANTS:\n",
    "    df[p] = pd.to_numeric(df[p], errors=\"coerce\")\n",
    "    df[p] = df.groupby(\"locationId\")[p].ffill().bfill()\n",
    "    if df[p].isna().any():\n",
    "        med_loc = df.groupby(\"locationId\")[p].transform(\"median\")\n",
    "        df[p] = df[p].fillna(med_loc)\n",
    "    if df[p].isna().any():\n",
    "        df[p] = df[p].fillna(df[p].median())\n",
    "\n",
    "# --- normalize units to EPA target units ---\n",
    "# use companion unit columns when present (e.g., pm25_unit, o3_unit)\n",
    "def normalize_series_to_target(p):\n",
    "    s = df[p].copy()\n",
    "    unit_col = f\"{p}_unit\"\n",
    "    u = df[unit_col].str.lower() if unit_col in df.columns else pd.Series(index=df.index, dtype=\"object\")\n",
    "\n",
    "    if p in (\"pm25\",\"pm10\"):\n",
    "        # EPA uses µg/m³ 24h avg; typically OpenAQ already µg/m³\n",
    "        # If someone reports ppm/ppb (rare), convert back to µg/m³ (not expected here) -> skip\n",
    "        return s\n",
    "\n",
    "    if p == \"o3\":  # target: ppm (8h max)\n",
    "        out = s.copy()\n",
    "        mask_ppm = u.str.contains(\"ppm\", na=False)\n",
    "        mask_ppb = u.str.contains(\"ppb\", na=False)\n",
    "        # assume remaining are µg/m³\n",
    "        out[mask_ppm] = s[mask_ppm]\n",
    "        out[mask_ppb] = s[mask_ppb] / 1000.0\n",
    "        out[~(mask_ppm | mask_ppb)] = s[~(mask_ppm | mask_ppb)].apply(lambda x: ugm3_to_ppm(x, \"o3\"))\n",
    "        return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "    if p == \"co\":  # target: ppm (8h max)\n",
    "        out = s.copy()\n",
    "        mask_ppm = u.str.contains(\"ppm\", na=False)\n",
    "        mask_ppb = u.str.contains(\"ppb\", na=False)\n",
    "        mask_mgm3 = u.str.contains(\"mg/m\", na=False)\n",
    "        out[mask_ppm] = s[mask_ppm]\n",
    "        out[mask_ppb] = s[mask_ppb] / 1000.0\n",
    "        out[mask_mgm3] = s[mask_mgm3] * 24.45 / MW[\"co\"]\n",
    "        out[~(mask_ppm | mask_ppb | mask_mgm3)] = s[~(mask_ppm | mask_ppb | mask_mgm3)].apply(lambda x: ugm3_to_ppm(x, \"co\"))\n",
    "        return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "    if p in (\"no2\",\"so2\"):  # target: ppb (1h max)\n",
    "        out = s.copy()\n",
    "        mask_ppb = u.str.contains(\"ppb\", na=False)\n",
    "        mask_ppm = u.str.contains(\"ppm\", na=False)\n",
    "        out[mask_ppb] = s[mask_ppb]\n",
    "        out[mask_ppm] = s[mask_ppm] * 1000.0\n",
    "        out[~(mask_ppb | mask_ppm)] = s[~(mask_ppb | mask_ppm)].apply(lambda x: ugm3_to_ppb(x, p))\n",
    "        return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "    return s\n",
    "\n",
    "std_cols = {}\n",
    "for p in POLLUTANTS:\n",
    "    std_cols[p] = f\"{p}_std\"\n",
    "    df[std_cols[p]] = normalize_series_to_target(p)\n",
    "\n",
    "# --- compute daily concentrations per EPA method ---\n",
    "df[\"date\"] = df[\"ts_hour\"].dt.floor(\"D\")\n",
    "\n",
    "daily_parts = []\n",
    "\n",
    "# PM2.5/PM10: 24h mean\n",
    "for p in (\"pm25\",\"pm10\"):\n",
    "    c = df.groupby([\"locationId\",\"date\"])[std_cols[p]].mean().reset_index()\n",
    "    c = c.rename(columns={std_cols[p]: f\"{p}_conc\"})\n",
    "    daily_parts.append(c)\n",
    "\n",
    "# O3/CO: daily max 8h rolling average\n",
    "for p in (\"o3\",\"co\"):\n",
    "    out_list = []\n",
    "    for loc, sub in df[[\"locationId\",\"ts_hour\",std_cols[p]]].dropna().groupby(\"locationId\"):\n",
    "        sub = sub.sort_values(\"ts_hour\").set_index(\"ts_hour\")\n",
    "        roll8 = sub[std_cols[p]].rolling(\"8H\", min_periods=6).mean().reset_index()\n",
    "        roll8[\"date\"] = roll8[\"ts_hour\"].dt.floor(\"D\")\n",
    "        dmax = roll8.groupby(\"date\")[std_cols[p]].max().reset_index().rename(columns={std_cols[p]: f\"{p}_conc\"})\n",
    "        dmax.insert(0, \"locationId\", loc)\n",
    "        out_list.append(dmax)\n",
    "    if out_list:\n",
    "        daily_parts.append(pd.concat(out_list, ignore_index=True))\n",
    "\n",
    "# NO2/SO2: daily max 1h\n",
    "for p in (\"no2\",\"so2\"):\n",
    "    c = df.groupby([\"locationId\",\"date\"])[std_cols[p]].max().reset_index()\n",
    "    c = c.rename(columns={std_cols[p]: f\"{p}_conc\"})\n",
    "    daily_parts.append(c)\n",
    "\n",
    "# merge all daily concs\n",
    "daily = None\n",
    "for part in daily_parts:\n",
    "    daily = part if daily is None else pd.merge(daily, part, on=[\"locationId\",\"date\"], how=\"outer\")\n",
    "\n",
    "# --- compute sub-AQIs and overall AQI ---\n",
    "for p in POLLUTANTS:\n",
    "    conc_col = f\"{p}_conc\"\n",
    "    if conc_col in daily.columns:\n",
    "        daily[f\"aqi_{p}\"] = daily[conc_col].apply(lambda x: aqi_from_bp(p, x))\n",
    "\n",
    "aqi_cols = [c for c in daily.columns if c.startswith(\"aqi_\")]\n",
    "daily[\"aqi\"] = daily[aqi_cols].max(axis=1, skipna=True)\n",
    "\n",
    "# --- next 24h AQI per location ---\n",
    "daily = daily.sort_values([\"locationId\",\"date\"])\n",
    "daily[\"aqi_next_24h\"] = daily.groupby(\"locationId\")[\"aqi\"].shift(-1)\n",
    "\n",
    "# --- optional metadata: bring most recent lat/lon/location/city/country of the day ---\n",
    "meta_cols = [c for c in [\"location\",\"city\",\"country\",\"lat\",\"lon\"] if c in df.columns]\n",
    "if meta_cols:\n",
    "    meta = (df.sort_values(\"ts_hour\")\n",
    "              .groupby([\"locationId\",\"date\"])[meta_cols]\n",
    "              .agg(lambda s: s.dropna().iloc[-1] if not s.dropna().empty else np.nan)\n",
    "              .reset_index())\n",
    "    daily = pd.merge(daily, meta, on=[\"locationId\",\"date\"], how=\"left\")\n",
    "\n",
    "daily = daily.sort_values([\"locationId\",\"date\"]).reset_index(drop=True)\n",
    "daily.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"[✓] wrote {len(daily)} rows → {OUTPUT_CSV}\")\n",
    "print(daily.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac41b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] wrote 3814 rows → openaq_new_york_bbox_last_month_daily_aqi.csv\n",
      "   locationId                      date  pm25_conc  pm10_conc   o3_conc  \\\n",
      "0         384 2016-03-12 00:00:00+00:00   2.400000      7.265  0.027000   \n",
      "1         384 2016-03-26 00:00:00+00:00   2.400000      7.265  0.027000   \n",
      "2         384 2016-03-31 00:00:00+00:00   8.550000      7.265  0.027000   \n",
      "3         384 2016-04-01 00:00:00+00:00  10.411765      7.265  0.023833   \n",
      "4         384 2016-04-02 00:00:00+00:00   5.020000      7.265  0.023429   \n",
      "5         384 2016-04-03 00:00:00+00:00   4.700000      7.265  0.027000   \n",
      "6         384 2016-05-12 00:00:00+00:00  15.400000      7.265  0.027000   \n",
      "7         384 2016-05-21 00:00:00+00:00   6.600000      7.265  0.027000   \n",
      "8         384 2016-08-04 00:00:00+00:00   6.900000      7.265  0.027000   \n",
      "9         384 2016-08-15 00:00:00+00:00   5.800000      7.265  0.027000   \n",
      "\n",
      "   co_conc  no2_conc  so2_conc  aqi_pm25  aqi_pm10  ...  aqi_co  aqi_no2  \\\n",
      "0   0.0002      19.0       0.0      10.0       7.0  ...     0.0     18.0   \n",
      "1   0.0002      19.0       0.0      10.0       7.0  ...     0.0     18.0   \n",
      "2   0.0002      19.0       0.0      36.0       7.0  ...     0.0     18.0   \n",
      "3   0.0002      19.0       0.0      43.0       7.0  ...     0.0     18.0   \n",
      "4   0.0002      19.0       0.0      21.0       7.0  ...     0.0     18.0   \n",
      "5   0.0002      19.0       0.0      20.0       7.0  ...     0.0     18.0   \n",
      "6   0.0002      19.0       0.0      58.0       7.0  ...     0.0     18.0   \n",
      "7   0.0002      19.0       0.0      28.0       7.0  ...     0.0     18.0   \n",
      "8   0.0002      19.0       0.0      29.0       7.0  ...     0.0     18.0   \n",
      "9   0.0002      19.0       0.0      24.0       7.0  ...     0.0     18.0   \n",
      "\n",
      "   aqi_so2   aqi  aqi_next_24h  location city  country      lat      lon  \n",
      "0      0.0  25.0          25.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "1      0.0  25.0          36.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "2      0.0  36.0          43.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "3      0.0  43.0          22.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "4      0.0  22.0          25.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "5      0.0  25.0          58.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "6      0.0  58.0          28.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "7      0.0  28.0          29.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "8      0.0  29.0          25.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "9      0.0  25.0          37.0      CCNY  NaN      NaN  40.8197 -73.9481  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== USER SETTINGS =====\n",
    "INPUT_CSV  = \"openaq_new_york_bbox_last_month_hourly_wide.csv\"\n",
    "OUTPUT_CSV = Path(INPUT_CSV).with_name(Path(INPUT_CSV).stem.replace(\"_hourly_wide\",\"\") + \"_daily_aqi.csv\")\n",
    "\n",
    "POLLUTANTS = [\"pm25\",\"pm10\",\"o3\",\"co\",\"no2\",\"so2\"]\n",
    "UNIT_COLS  = [f\"{p}_unit\" for p in POLLUTANTS]\n",
    "\n",
    "# molecular weights (25°C, 1 atm; 24.45 L/mol)\n",
    "MW = {\"o3\": 48.00, \"co\": 28.01, \"no2\": 46.01, \"so2\": 64.07}\n",
    "\n",
    "# EPA AQI breakpoints\n",
    "AQI_BP = {\n",
    "    \"pm25\": [(0.0,12.0,0,50),(12.1,35.4,51,100),(35.5,55.4,101,150),(55.5,150.4,151,200),(150.5,250.4,201,300),(250.5,350.4,301,400),(350.5,500.4,401,500)],\n",
    "    \"pm10\": [(0,54,0,50),(55,154,51,100),(155,254,101,150),(255,354,151,200),(355,424,201,300),(425,504,301,400),(505,604,401,500)],\n",
    "    \"o3\":   [(0.000,0.054,0,50),(0.055,0.070,51,100),(0.071,0.085,101,150),(0.086,0.105,151,200),(0.106,0.200,201,300)],\n",
    "    \"co\":   [(0.0,4.4,0,50),(4.5,9.4,51,100),(9.5,12.4,101,150),(12.5,15.4,151,200),(15.5,30.4,201,300),(30.5,40.4,301,400),(40.5,50.4,401,500)],\n",
    "    \"no2\":  [(0,53,0,50),(54,100,51,100),(101,360,101,150),(361,649,151,200),(650,1249,201,300),(1250,1649,301,400),(1650,2049,401,500)],\n",
    "    \"so2\":  [(0,35,0,50),(36,75,51,100),(76,185,101,150),(186,304,151,200),(305,604,201,300),(605,804,301,400),(805,1004,401,500)],\n",
    "}\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def aqi_from_bp(p, conc):\n",
    "    if pd.isna(conc): return np.nan\n",
    "    for (Cl, Ch, Il, Ih) in AQI_BP[p]:\n",
    "        if Cl <= conc <= Ch:\n",
    "            return (Ih-Il)/(Ch-Cl)*(conc-Cl)+Il\n",
    "    return np.nan\n",
    "\n",
    "def ugm3_to_ppm(x, species):  # µg/m³ -> ppm\n",
    "    if pd.isna(x): return np.nan\n",
    "    return (x * 24.45) / (MW[species] * 1000.0)\n",
    "\n",
    "def ugm3_to_ppb(x, species):  # µg/m³ -> ppb\n",
    "    if pd.isna(x): return np.nan\n",
    "    return (x * 24.45) / MW[species]\n",
    "\n",
    "def canon_unit(raw):\n",
    "    if raw is None or (isinstance(raw,float) and np.isnan(raw)): return \"\"\n",
    "    u = str(raw).lower()\n",
    "    u = re.sub(r\"[^a-z0-9/µ³]\", \"\", u)  # strip commas/whitespace/etc.\n",
    "    u = u.replace(\"μ\", \"µ\")\n",
    "    u = u.replace(\"ug/m3\",\"µg/m3\").replace(\"ugm3\",\"µg/m3\").replace(\"µgm3\",\"µg/m3\").replace(\"mug/m3\",\"µg/m3\")\n",
    "    return u\n",
    "\n",
    "def ensure_ts_hour(df):\n",
    "    if \"ts_hour\" in df.columns:\n",
    "        df[\"ts_hour\"] = pd.to_datetime(df[\"ts_hour\"], utc=True, errors=\"coerce\").dt.floor(\"H\")\n",
    "    else:\n",
    "        ts_col = \"ts_utc\" if \"ts_utc\" in df.columns else None\n",
    "        if ts_col is None:\n",
    "            raise ValueError(\"No timestamp column found (ts_hour or ts_utc).\")\n",
    "        df[\"ts_hour\"] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\").dt.floor(\"H\")\n",
    "    return df\n",
    "\n",
    "# ---------- load robustly ----------\n",
    "first = pd.read_csv(INPUT_CSV, nrows=1)\n",
    "dtype_units = {c: \"string\" for c in UNIT_COLS if c in first.columns}\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False, dtype=dtype_units)\n",
    "df = ensure_ts_hour(df)\n",
    "\n",
    "# ensure locationId\n",
    "if \"locationId\" not in df.columns:\n",
    "    if {\"lat\",\"lon\"} <= set(df.columns):\n",
    "        df[\"locationId\"] = (df[\"lat\"].round(5).astype(str) + \",\" + df[\"lon\"].round(5).astype(str))\n",
    "    else:\n",
    "        df[\"locationId\"] = \"unknown\"\n",
    "\n",
    "# ensure all pollutant & unit columns exist\n",
    "for p in POLLUTANTS:\n",
    "    if p not in df.columns: df[p] = np.nan\n",
    "for c in UNIT_COLS:\n",
    "    if c not in df.columns: df[c] = \"\"\n",
    "\n",
    "# ---------- impute hourly values (per location), guarantee no NaNs ----------\n",
    "df = df.sort_values([\"locationId\",\"ts_hour\"]).reset_index(drop=True)\n",
    "for p in POLLUTANTS:\n",
    "    df[p] = pd.to_numeric(df[p], errors=\"coerce\")\n",
    "    # per-location ffill/bfill\n",
    "    df[p] = df.groupby(\"locationId\", group_keys=False)[p].apply(lambda s: s.ffill().bfill())\n",
    "    # per-location median\n",
    "    loc_med = df.groupby(\"locationId\")[p].transform(\"median\")\n",
    "    df[p] = df[p].fillna(loc_med)\n",
    "    # global median\n",
    "    glob_med = df[p].median()\n",
    "    df[p] = df[p].fillna(glob_med)\n",
    "    # final fallback\n",
    "    df[p] = df[p].fillna(0.0)\n",
    "\n",
    "# ---------- normalize units to EPA targets (tolerant parsing + heuristics) ----------\n",
    "for c in UNIT_COLS:\n",
    "    df[c] = df[c].astype(\"string\").map(canon_unit)\n",
    "\n",
    "def norm_to_target(p):\n",
    "    s = pd.to_numeric(df[p], errors=\"coerce\")\n",
    "    u = df[f\"{p}_unit\"]\n",
    "    if p in (\"pm25\",\"pm10\"):\n",
    "        return s  # µg/m³ already the target for daily avg\n",
    "    if p == \"o3\":\n",
    "        out = s.copy()\n",
    "        m_ppm = u.str.contains(\"ppm\", na=False)\n",
    "        m_ppb = u.str.contains(\"ppb\", na=False)\n",
    "        m_ug  = u.str.contains(\"µg/m3\", na=False)\n",
    "        out[m_ppm] = s[m_ppm]\n",
    "        out[m_ppb] = s[m_ppb] / 1000.0\n",
    "        out[m_ug]  = s[m_ug].apply(lambda x: ugm3_to_ppm(x,\"o3\"))\n",
    "        unk = ~(m_ppm|m_ppb|m_ug)\n",
    "        out[unk & (s < 0.2)] = s[unk & (s < 0.2)]           # looks like ppm\n",
    "        out[unk & (s > 200)] = s[unk & (s > 200)].apply(lambda x: ugm3_to_ppm(x,\"o3\"))\n",
    "        mid = unk & (s >= 0.2) & (s <= 200)                 # looks like ppb\n",
    "        out[mid] = s[mid] / 1000.0\n",
    "        return pd.to_numeric(out, errors=\"coerce\")\n",
    "    if p == \"co\":\n",
    "        out = s.copy()\n",
    "        m_ppm = u.str.contains(\"ppm\", na=False)\n",
    "        m_ppb = u.str.contains(\"ppb\", na=False)\n",
    "        m_mg  = u.str.contains(\"mg/m3\", na=False)\n",
    "        m_ug  = u.str.contains(\"µg/m3\", na=False)\n",
    "        out[m_ppm] = s[m_ppm]\n",
    "        out[m_ppb] = s[m_ppb] / 1000.0\n",
    "        out[m_mg]  = s[m_mg] * 24.45 / MW[\"co\"]\n",
    "        out[m_ug]  = s[m_ug].apply(lambda x: ugm3_to_ppm(x,\"co\"))\n",
    "        unk = ~(m_ppm|m_ppb|m_mg|m_ug)\n",
    "        out[unk & (s < 0.2)] = s[unk & (s < 0.2)]\n",
    "        out[unk & (s > 200)] = s[unk & (s > 200)].apply(lambda x: ugm3_to_ppm(x,\"co\"))\n",
    "        mid = unk & (s >= 0.2) & (s <= 200)\n",
    "        out[mid] = s[mid] / 1000.0\n",
    "        return pd.to_numeric(out, errors=\"coerce\")\n",
    "    if p in (\"no2\",\"so2\"):\n",
    "        out = s.copy()\n",
    "        mw = MW[p]\n",
    "        m_ppb = u.str.contains(\"ppb\", na=False)\n",
    "        m_ppm = u.str.contains(\"ppm\", na=False)\n",
    "        m_ug  = u.str.contains(\"µg/m3\", na=False)\n",
    "        out[m_ppb] = s[m_ppb]\n",
    "        out[m_ppm] = s[m_ppm] * 1000.0\n",
    "        out[m_ug]  = s[m_ug].apply(lambda x: ugm3_to_ppb(x,p))\n",
    "        unk = ~(m_ppb|m_ppm|m_ug)\n",
    "        out[unk & (s < 0.2)] = s[unk & (s < 0.2)] * 1000.0   # ppm-ish → ppb\n",
    "        out[unk & (s > 200)] = s[unk & (s > 200)].apply(lambda x: ugm3_to_ppb(x,p))\n",
    "        mid = unk & (s >= 0.2) & (s <= 200)\n",
    "        out[mid] = s[mid]                                     # ppb-ish\n",
    "        return pd.to_numeric(out, errors=\"coerce\")\n",
    "    return s\n",
    "\n",
    "std_cols = {}\n",
    "for p in POLLUTANTS:\n",
    "    std_cols[p] = f\"{p}_std\"\n",
    "    df[std_cols[p]] = norm_to_target(p)\n",
    "    # make sure no NaNs after normalization\n",
    "    df[std_cols[p]] = df[std_cols[p]].fillna(df.groupby(\"locationId\")[std_cols[p]].transform(\"median\"))\n",
    "    df[std_cols[p]] = df[std_cols[p]].fillna(df[std_cols[p]].median()).fillna(0.0)\n",
    "\n",
    "# ---------- daily concentrations ----------\n",
    "df[\"date\"] = df[\"ts_hour\"].dt.floor(\"D\")\n",
    "daily_parts = []\n",
    "\n",
    "# PM: 24h mean\n",
    "for p in (\"pm25\",\"pm10\"):\n",
    "    c = df.groupby([\"locationId\",\"date\"])[std_cols[p]].mean().reset_index()\n",
    "    c = c.rename(columns={std_cols[p]: f\"{p}_conc\"})\n",
    "    daily_parts.append(c)\n",
    "\n",
    "# O3/CO: max 8h rolling average\n",
    "for p in (\"o3\",\"co\"):\n",
    "    out_list = []\n",
    "    for loc, sub in df[[\"locationId\",\"ts_hour\",std_cols[p]]].groupby(\"locationId\"):\n",
    "        s = sub.sort_values(\"ts_hour\").set_index(\"ts_hour\")[std_cols[p]]\n",
    "        roll8 = s.rolling(\"8H\", min_periods=6).mean().reset_index(name=\"avg8\")\n",
    "        roll8[\"date\"] = roll8[\"ts_hour\"].dt.floor(\"D\")\n",
    "        daymax = roll8.groupby(\"date\")[\"avg8\"].max().reset_index().rename(columns={\"avg8\": f\"{p}_conc\"})\n",
    "        daymax.insert(0, \"locationId\", loc)\n",
    "        out_list.append(daymax)\n",
    "    if out_list:\n",
    "        daily_parts.append(pd.concat(out_list, ignore_index=True))\n",
    "\n",
    "# NO2/SO2: max 1h\n",
    "for p in (\"no2\",\"so2\"):\n",
    "    c = df.groupby([\"locationId\",\"date\"])[std_cols[p]].max().reset_index()\n",
    "    c = c.rename(columns={std_cols[p]: f\"{p}_conc\"})\n",
    "    daily_parts.append(c)\n",
    "\n",
    "# merge & fill remaining holes (use per-pollutant medians, then 0)\n",
    "daily = None\n",
    "for part in daily_parts:\n",
    "    daily = part if daily is None else pd.merge(daily, part, on=[\"locationId\",\"date\"], how=\"outer\")\n",
    "\n",
    "for p in POLLUTANTS:\n",
    "    col = f\"{p}_conc\"\n",
    "    if col in daily.columns:\n",
    "        med = daily[col].median(skipna=True)\n",
    "        daily[col] = daily[col].fillna(med).fillna(0.0)\n",
    "\n",
    "# ---------- AQI ----------\n",
    "for p in POLLUTANTS:\n",
    "    col = f\"{p}_conc\"\n",
    "    if col in daily.columns:\n",
    "        daily[f\"aqi_{p}\"] = daily[col].apply(lambda x: aqi_from_bp(p, x))\n",
    "\n",
    "aqi_cols = [c for c in daily.columns if c.startswith(\"aqi_\")]\n",
    "# If all sub-AQIs are NaN for a row (should be rare), set overall AQI to 0\n",
    "daily[\"aqi\"] = daily[aqi_cols].max(axis=1, skipna=True).fillna(0.0)\n",
    "for c in aqi_cols + [\"aqi\"]:\n",
    "    daily[c] = daily[c].round(0)\n",
    "\n",
    "# next 24h AQI (last day per location will be NaN; keep as-is)\n",
    "daily = daily.sort_values([\"locationId\",\"date\"])\n",
    "daily[\"aqi_next_24h\"] = daily.groupby(\"locationId\")[\"aqi\"].shift(-1)\n",
    "\n",
    "# ---------- optional metadata backfill ----------\n",
    "meta_cols = [c for c in [\"location\",\"city\",\"country\",\"lat\",\"lon\"] if c in df.columns]\n",
    "if meta_cols:\n",
    "    meta = (df.sort_values(\"ts_hour\")\n",
    "              .groupby([\"locationId\",\"date\"])[meta_cols]\n",
    "              .agg(lambda s: s.dropna().iloc[-1] if not s.dropna().empty else np.nan)\n",
    "              .reset_index())\n",
    "    daily = pd.merge(daily, meta, on=[\"locationId\",\"date\"], how=\"left\")\n",
    "\n",
    "daily = daily.sort_values([\"locationId\",\"date\"]).reset_index(drop=True)\n",
    "daily.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"[✓] wrote {len(daily)} rows → {OUTPUT_CSV}\")\n",
    "print(daily.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec922f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Metrics (overall) ===\n",
      "MAE: 8.853\n",
      "RMSE: 13.684\n",
      "R2: 0.269\n",
      "Bias_ME: 1.206\n",
      "MedianAE: 6.010\n",
      "P90AE: 20.613\n",
      "MAPE_%: 24.693\n",
      "\n",
      "[✓] wrote validation predictions  → aqi_next_day_val_preds.csv\n",
      "[✓] wrote overall report         → aqi_next_day_val_report.csv\n",
      "[✓] wrote per-location report    → aqi_next_day_val_report_by_location.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] wrote permutation importances → aqi_next_day_val_feature_importances.csv\n",
      "\n",
      "Top 15 features by importance:\n",
      "                 feature  importance_mean  importance_std\n",
      "34        aqi_rollmean_7         1.409543        0.131847\n",
      "5                    doy         0.904514        0.145742\n",
      "12             aqi_lag_7         0.752854        0.086889\n",
      "32  pm25_conc_rollmean_3         0.415545        0.039979\n",
      "13       pm25_conc_lag_1         0.333446        0.057800\n",
      "9              aqi_lag_4         0.333243        0.049326\n",
      "35  pm25_conc_rollmean_7         0.331214        0.083048\n",
      "38                   lon         0.283432        0.058616\n",
      "8              aqi_lag_3         0.265166        0.020334\n",
      "6              aqi_lag_1         0.242031        0.078718\n",
      "14       pm25_conc_lag_2         0.223337        0.031796\n",
      "11             aqi_lag_6         0.210221        0.047717\n",
      "15       pm25_conc_lag_3         0.167005        0.042232\n",
      "37                   lat         0.142016        0.034606\n",
      "31        aqi_rollmean_3         0.114049        0.038891\n",
      "\n",
      "[✓] saved model → aqi_next_day_hgb.joblib\n",
      "\n",
      "Next-day AQI predictions (per location):\n",
      "      locationId       date  aqi_pred_next_24h\n",
      "1810         984 2016-04-23          58.441556\n",
      "2063        2193 2016-04-25          26.896719\n",
      "1773         974 2016-04-30          36.349655\n",
      "1612         924 2016-05-06          29.398467\n",
      "1476         853 2016-05-17          31.388573\n",
      "1305         665 2016-12-17          34.540998\n",
      "511          626 2016-12-18          32.071565\n",
      "971          648 2016-12-18          37.800985\n",
      "1193         664 2016-12-19          39.785079\n",
      "1080         662 2016-12-19          40.641483\n"
     ]
    }
   ],
   "source": [
    "# train_aqi_next_day.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "# (optional) silence joblib CPU warning on Windows – set to your physical cores\n",
    "os.environ.setdefault(\"LOKY_MAX_CPU_COUNT\", \"8\")\n",
    "\n",
    "# ========= USER SETTINGS =========\n",
    "INPUT_CSV = \"openaq_new_york_bbox_last_month_daily_aqi.csv\"\n",
    "OUT_DIR = Path(\".\")\n",
    "MODEL_PATH = OUT_DIR / \"aqi_next_day_hgb.joblib\"\n",
    "VAL_PREDS_CSV = OUT_DIR / \"aqi_next_day_val_preds.csv\"\n",
    "VAL_REPORT_CSV = OUT_DIR / \"aqi_next_day_val_report.csv\"\n",
    "VAL_PER_SITE_CSV = OUT_DIR / \"aqi_next_day_val_report_by_location.csv\"\n",
    "VAL_IMPORTANCES_CSV = OUT_DIR / \"aqi_next_day_val_feature_importances.csv\"\n",
    "\n",
    "# which features to lag / roll\n",
    "LAG_AQI_DAYS = [1,2,3,4,5,6,7]\n",
    "LAG_POLLUTANT_DAYS = [1,2,3]\n",
    "ROLL_WINDOWS = [3,7]  # in days\n",
    "ROLL_VARS = [\"aqi\",\"pm25_conc\",\"o3_conc\"]  # small & predictive\n",
    "\n",
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"dow\"] = df[\"date\"].dt.weekday\n",
    "    df[\"is_weekend\"] = df[\"dow\"].isin([5,6]).astype(int)\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"doy\"] = df[\"date\"].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "def make_lags_rolls(df):\n",
    "    df = df.sort_values([\"locationId\",\"date\"]).copy()\n",
    "    by_loc = []\n",
    "    for loc, g in df.groupby(\"locationId\"):\n",
    "        g = g.sort_values(\"date\").copy()\n",
    "\n",
    "        for k in LAG_AQI_DAYS:\n",
    "            g[f\"aqi_lag_{k}\"] = g[\"aqi\"].shift(k)\n",
    "\n",
    "        pols = [\"pm25_conc\",\"pm10_conc\",\"o3_conc\",\"co_conc\",\"no2_conc\",\"so2_conc\"]\n",
    "        for p in pols:\n",
    "            if p in g.columns:\n",
    "                for k in LAG_POLLUTANT_DAYS:\n",
    "                    g[f\"{p}_lag_{k}\"] = g[p].shift(k)\n",
    "\n",
    "        for w in ROLL_WINDOWS:\n",
    "            for v in ROLL_VARS:\n",
    "                if v in g.columns:\n",
    "                    g[f\"{v}_rollmean_{w}\"] = (\n",
    "                        g[v].rolling(window=w, min_periods=max(1, int(np.ceil(w*0.6))))\n",
    "                            .mean()\n",
    "                            .shift(1)\n",
    "                    )\n",
    "        by_loc.append(g)\n",
    "    return pd.concat(by_loc, ignore_index=True)\n",
    "\n",
    "def build_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(\"Expected a 'date' column.\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "\n",
    "    for c in [\"locationId\",\"date\",\"aqi\",\"aqi_next_24h\"]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "    pol_cols = [\"pm25_conc\",\"pm10_conc\",\"o3_conc\",\"co_conc\",\"no2_conc\",\"so2_conc\"]\n",
    "    for c in pol_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    for meta in [\"lat\",\"lon\",\"city\",\"country\",\"location\"]:\n",
    "        if meta not in df.columns:\n",
    "            df[meta] = np.nan\n",
    "\n",
    "    df = df.sort_values([\"locationId\",\"date\"])\n",
    "    for c in pol_cols + [\"aqi\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df[c] = df.groupby(\"locationId\")[c].ffill().bfill()\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "    df = add_time_features(df)\n",
    "    df = make_lags_rolls(df)\n",
    "\n",
    "    feature_cols = [col for col in df.columns if any(s in col for s in [\n",
    "        \"lag_\",\"rollmean_\",\"dow\",\"month\",\"is_weekend\",\"doy\"\n",
    "    ])] + [\"lat\",\"lon\"]\n",
    "    X = df[[\"locationId\",\"date\"] + feature_cols].copy()\n",
    "    y = df[\"aqi_next_24h\"].copy()\n",
    "\n",
    "    keep = y.notna()\n",
    "    for c in feature_cols:\n",
    "        keep &= X[c].notna()\n",
    "    X = X[keep].reset_index(drop=True)\n",
    "    y = y[keep].reset_index(drop=True)\n",
    "\n",
    "    return df, X, y, feature_cols\n",
    "\n",
    "def time_based_split(X, y, frac_val=0.2):\n",
    "    Xy = X.copy()\n",
    "    Xy[\"target\"] = y.values\n",
    "    val_idx, train_idx = [], []\n",
    "    for _, g in Xy.groupby(\"locationId\"):\n",
    "        n = len(g)\n",
    "        cut = int(np.floor(n * (1 - frac_val)))\n",
    "        idx_sorted = g.sort_values(\"date\").index\n",
    "        train_idx.extend(idx_sorted[:cut])\n",
    "        val_idx.extend(idx_sorted[cut:])\n",
    "    return sorted(set(train_idx)), sorted(set(val_idx))\n",
    "\n",
    "def regression_report(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    me = float(np.mean(y_pred - y_true))            # bias\n",
    "    med_ae = float(np.median(np.abs(y_pred - y_true)))\n",
    "    p90_ae = float(np.percentile(np.abs(y_pred - y_true), 90))\n",
    "    # MAPE with protection against division by zero\n",
    "    denom = np.where(y_true == 0, np.nan, np.abs(y_true))\n",
    "    mape = float(np.nanmean(np.abs((y_pred - y_true) / denom)) * 100.0)\n",
    "    return {\n",
    "        \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2,\n",
    "        \"Bias_ME\": me, \"MedianAE\": med_ae, \"P90AE\": p90_ae, \"MAPE_%\": mape\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    df_all, X, y, feature_cols = build_dataset(INPUT_CSV)\n",
    "\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[(\"num\", \"passthrough\", feature_cols)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_depth=None,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=500,\n",
    "        l2_regularization=0.0,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"prep\", preproc), (\"model\", model)])\n",
    "\n",
    "    # time-based split\n",
    "    train_idx, val_idx = time_based_split(X, y, frac_val=0.2)\n",
    "    X_train, y_train = X.loc[train_idx, :], y.loc[train_idx]\n",
    "    X_val,   y_val   = X.loc[val_idx, :],   y.loc[val_idx]\n",
    "\n",
    "    # fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # -------- VALIDATION: predictions + metrics ----------\n",
    "    pred_val = pipe.predict(X_val)\n",
    "    overall = regression_report(y_val.values, pred_val)\n",
    "\n",
    "    print(\"\\n=== Validation Metrics (overall) ===\")\n",
    "    for k, v in overall.items():\n",
    "        if np.isnan(v):\n",
    "            print(f\"{k}: NaN\")\n",
    "        else:\n",
    "            print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "    # per-location stats\n",
    "    per_site_rows = []\n",
    "    Xy_val = X_val[[\"locationId\",\"date\"]].copy()\n",
    "    Xy_val[\"y_true\"] = y_val.values\n",
    "    Xy_val[\"y_pred\"] = pred_val\n",
    "    for loc, g in Xy_val.groupby(\"locationId\"):\n",
    "        rep = regression_report(g[\"y_true\"].values, g[\"y_pred\"].values)\n",
    "        rep[\"locationId\"] = loc\n",
    "        rep[\"n_samples\"] = len(g)\n",
    "        per_site_rows.append(rep)\n",
    "    per_site = pd.DataFrame(per_site_rows).sort_values(\"MAE\")\n",
    "\n",
    "    # save predictions & reports\n",
    "    Xy_val.to_csv(VAL_PREDS_CSV, index=False)\n",
    "    pd.DataFrame([overall]).to_csv(VAL_REPORT_CSV, index=False)\n",
    "    per_site.to_csv(VAL_PER_SITE_CSV, index=False)\n",
    "    print(f\"\\n[✓] wrote validation predictions  → {VAL_PREDS_CSV}\")\n",
    "    print(f\"[✓] wrote overall report         → {VAL_REPORT_CSV}\")\n",
    "    print(f\"[✓] wrote per-location report    → {VAL_PER_SITE_CSV}\")\n",
    "\n",
    "    # -------- Permutation feature importance (on validation) ----------\n",
    "    # Use the transformed numeric features\n",
    "    feat_names = pipe.named_steps[\"prep\"].feature_names_in_\n",
    "    r = permutation_importance(\n",
    "        pipe, X_val, y_val, n_repeats=10, random_state=42, n_jobs=1, scoring=\"neg_mean_absolute_error\"\n",
    "    )\n",
    "    importances = (pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"importance_mean\": r.importances_mean,\n",
    "        \"importance_std\": r.importances_std\n",
    "    })\n",
    "    .sort_values(\"importance_mean\", ascending=False))\n",
    "    importances.to_csv(VAL_IMPORTANCES_CSV, index=False)\n",
    "    print(f\"[✓] wrote permutation importances → {VAL_IMPORTANCES_CSV}\")\n",
    "    print(\"\\nTop 15 features by importance:\")\n",
    "    print(importances.head(15))\n",
    "\n",
    "    # -------- Retrain on ALL data and save model ----------\n",
    "    pipe.fit(X, y)\n",
    "    joblib.dump({\"pipeline\": pipe, \"features\": feature_cols}, MODEL_PATH)\n",
    "    print(f\"\\n[✓] saved model → {MODEL_PATH}\")\n",
    "\n",
    "    # -------- Predict next-day AQI for each location (t -> t+1) ----------\n",
    "    X_latest = (\n",
    "        X.sort_values(\"date\")\n",
    "         .groupby(\"locationId\", as_index=False)\n",
    "         .tail(1)\n",
    "         .copy()\n",
    "    )\n",
    "    preds_next = pipe.predict(X_latest[feat_names])\n",
    "    next_out = X_latest[[\"locationId\",\"date\"]].copy()\n",
    "    next_out[\"date\"] = next_out[\"date\"] + pd.Timedelta(days=1)\n",
    "    next_out[\"aqi_pred_next_24h\"] = preds_next\n",
    "    print(\"\\nNext-day AQI predictions (per location):\")\n",
    "    print(next_out.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5570b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 40 candidates, totalling 40 fits\n",
      "Best params: {'model__l2_regularization': 0.0005981457917250761, 'model__learning_rate': 0.03573884768117847, 'model__max_depth': 8, 'model__max_iter': 505, 'model__min_samples_leaf': 13}\n",
      "\n",
      "=== Validation Metrics (tuned) ===\n",
      "MAE: 5.230\n",
      "RMSE: 7.679\n",
      "R2: 0.747\n",
      "Bias_ME: 0.204\n",
      "MedianAE: 4.100\n",
      "P90AE: 11.160\n",
      "MAPE_%: 14.800\n",
      "\n",
      "=== Validation Metrics (tuned + per-site bias) ===\n",
      "MAE: 4.551\n",
      "RMSE: 6.969\n",
      "R2: 0.792\n",
      "Bias_ME: -0.000\n",
      "MedianAE: 3.155\n",
      "P90AE: 10.591\n",
      "MAPE_%: 12.731\n",
      "\n",
      "Top 15 features:\n",
      "                 feature  importance_mean  importance_std\n",
      "72            aqi_ewm_14         1.206804        0.090983\n",
      "7                cos_doy         1.004292        0.099374\n",
      "74       pm25_conc_ewm_7         0.952531        0.077934\n",
      "75      pm25_conc_ewm_14         0.909454        0.076377\n",
      "56       aqi_rollmean_14         0.695752        0.045391\n",
      "13             aqi_lag_4         0.390161        0.047443\n",
      "10             aqi_lag_1         0.387077        0.045892\n",
      "55         aqi_rollstd_7         0.382825        0.025087\n",
      "24       pm25_conc_lag_1         0.339075        0.036205\n",
      "29       pm25_conc_lag_6         0.339046        0.033930\n",
      "2                    dow         0.329277        0.029507\n",
      "61   pm25_conc_rollstd_7         0.318262        0.032899\n",
      "5                    doy         0.285718        0.031843\n",
      "60  pm25_conc_rollmean_7         0.269415        0.022723\n",
      "33      pm25_conc_lag_10         0.259910        0.021023\n",
      "\n",
      "[✓] saved tuned model → aqi_next_day_hgb_tuned.joblib\n",
      "\n",
      "Next-day AQI predictions (per location, raw & bias-adjusted):\n",
      "      locationId       date  aqi_pred_next_24h  aqi_pred_next_24h_biasadj\n",
      "1677         984 2016-04-23          58.147299                  69.294106\n",
      "1909        2193 2016-04-25          27.300670                  26.856187\n",
      "1647         974 2016-04-30          40.739253                  40.501929\n",
      "1507         924 2016-05-06          33.017304                  27.902857\n",
      "1385         853 2016-05-17          31.275163                  30.244475\n",
      "1228         665 2016-12-17          38.070511                  36.312927\n",
      "915          648 2016-12-18          37.893804                  37.354832\n",
      "483          626 2016-12-18          31.835948                  29.333039\n",
      "1123         664 2016-12-19          36.213565                  34.498883\n",
      "1017         662 2016-12-19          36.339947                  35.332527\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform, randint\n",
    "import joblib\n",
    "\n",
    "os.environ.setdefault(\"LOKY_MAX_CPU_COUNT\", \"8\")\n",
    "\n",
    "# ================== USER SETTINGS ==================\n",
    "INPUT_CSV   = \"openaq_new_york_bbox_last_month_daily_aqi.csv\"\n",
    "WEATHER_CSV = None\n",
    "OUT_DIR     = Path(\".\")\n",
    "\n",
    "MODEL_PATH            = OUT_DIR / \"aqi_next_day_hgb_tuned.joblib\"\n",
    "VAL_PREDS_CSV         = OUT_DIR / \"aqi_next_day_val_preds.csv\"\n",
    "VAL_REPORT_CSV        = OUT_DIR / \"aqi_next_day_val_report.csv\"\n",
    "VAL_PER_SITE_CSV      = OUT_DIR / \"aqi_next_day_val_report_by_location.csv\"\n",
    "VAL_IMPORTANCES_CSV   = OUT_DIR / \"aqi_next_day_val_feature_importances.csv\"\n",
    "\n",
    "LAG_AQI_DAYS        = list(range(1, 15))\n",
    "LAG_POLLUTANT_DAYS  = [1, 2, 3, 7]\n",
    "ROLL_WINDOWS        = [3, 7, 14]\n",
    "EWMA_SPANS          = [3, 7, 14]\n",
    "\n",
    "POLS = [\"pm25_conc\",\"pm10_conc\",\"o3_conc\",\"co_conc\",\"no2_conc\",\"so2_conc\"]\n",
    "WEATHER_VARS = [\"temp\",\"rh\",\"wind_speed\",\"wind_dir\",\"precip\",\"pblh\"]\n",
    "\n",
    "def add_calendar_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"dow\"] = df[\"date\"].dt.weekday\n",
    "    df[\"is_weekend\"] = df[\"dow\"].isin([5,6]).astype(int)\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"doy\"] = df[\"date\"].dt.dayofyear\n",
    "    tau = 2 * math.pi\n",
    "    df[\"sin_doy\"] = np.sin(tau * df[\"doy\"] / 365.25)\n",
    "    df[\"cos_doy\"] = np.cos(tau * df[\"doy\"] / 365.25)\n",
    "    return df\n",
    "\n",
    "def add_sequence_features(df, cols_for_lags, lags, roll_windows, ewma_spans):\n",
    "    df = df.sort_values([\"locationId\",\"date\"]).copy()\n",
    "    out = []\n",
    "    for _, g in df.groupby(\"locationId\"):\n",
    "        g = g.sort_values(\"date\").copy()\n",
    "        for c in cols_for_lags:\n",
    "            if c not in g.columns: \n",
    "                continue\n",
    "            for k in lags:\n",
    "                g[f\"{c}_lag_{k}\"] = g[c].shift(k)\n",
    "        for c in cols_for_lags:\n",
    "            if c not in g.columns: \n",
    "                continue\n",
    "            for w in roll_windows:\n",
    "                gm = g[c].rolling(window=w, min_periods=max(1,int(np.ceil(w*0.6)))).mean().shift(1)\n",
    "                gs = g[c].rolling(window=w, min_periods=max(1,int(np.ceil(w*0.6)))).std().shift(1)\n",
    "                g[f\"{c}_rollmean_{w}\"] = gm\n",
    "                g[f\"{c}_rollstd_{w}\"]  = gs\n",
    "        for c in cols_for_lags:\n",
    "            if c not in g.columns: \n",
    "                continue\n",
    "            for s in ewma_spans:\n",
    "                g[f\"{c}_ewm_{s}\"] = g[c].ewm(span=s, adjust=False, min_periods=max(1,int(np.ceil(s*0.6)))).mean().shift(1)\n",
    "        out.append(g)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "def regression_report(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    me   = float(np.mean(y_pred - y_true))\n",
    "    med_ae = float(np.median(np.abs(y_pred - y_true)))\n",
    "    p90_ae = float(np.percentile(np.abs(y_pred - y_true), 90))\n",
    "    denom  = np.where(y_true == 0, np.nan, np.abs(y_true))\n",
    "    mape = float(np.nanmean(np.abs((y_pred - y_true) / denom)) * 100.0)\n",
    "    return {\"MAE\":mae,\"RMSE\":rmse,\"R2\":r2,\"Bias_ME\":me,\"MedianAE\":med_ae,\"P90AE\":p90_ae,\"MAPE_%\":mape}\n",
    "\n",
    "def time_split_indices(X, y, frac_val=0.2):\n",
    "    Xy = X.copy()\n",
    "    Xy[\"target\"] = y.values\n",
    "    train_idx, val_idx = [], []\n",
    "    for _, g in Xy.groupby(\"locationId\"):\n",
    "        n = len(g)\n",
    "        if n < 5:\n",
    "            train_idx.extend(g.index)\n",
    "            continue\n",
    "        cut = int(np.floor(n * (1 - frac_val)))\n",
    "        idx_sorted = g.sort_values(\"date\").index\n",
    "        train_idx.extend(idx_sorted[:cut])\n",
    "        val_idx.extend(idx_sorted[cut:])\n",
    "    return sorted(set(train_idx)), sorted(set(val_idx))\n",
    "\n",
    "def build_dataset(aqi_path, weather_path=None):\n",
    "    df = pd.read_csv(aqi_path)\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'date' column in AQI CSV.\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "\n",
    "    for c in [\"locationId\",\"date\",\"aqi\",\"aqi_next_24h\"]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing column: {c}\")\n",
    "\n",
    "    for c in POLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    for c in [\"lat\",\"lon\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    if weather_path:\n",
    "        w = pd.read_csv(weather_path)\n",
    "        if \"date\" not in w.columns or \"locationId\" not in w.columns:\n",
    "            raise ValueError(\"Weather CSV must have 'locationId' and 'date'.\")\n",
    "        w[\"date\"] = pd.to_datetime(w[\"date\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "        keep_w = [\"locationId\",\"date\"] + [c for c in WEATHER_VARS if c in w.columns]\n",
    "        w = w[keep_w].copy()\n",
    "        df = pd.merge(df, w, on=[\"locationId\",\"date\"], how=\"left\")\n",
    "\n",
    "    df = df.sort_values([\"locationId\",\"date\"])\n",
    "    for c in POLS + [\"aqi\"] + [c for c in WEATHER_VARS if c in df.columns]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df[c] = df.groupby(\"locationId\")[c].ffill().bfill()\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "    df = add_calendar_features(df)\n",
    "\n",
    "    cols_for_lags = [\"aqi\",\"pm25_conc\",\"o3_conc\"] + [c for c in WEATHER_VARS if c in df.columns]\n",
    "    df = add_sequence_features(df, cols_for_lags, LAG_AQI_DAYS, ROLL_WINDOWS, EWMA_SPANS)\n",
    "    df = add_sequence_features(df, POLS, LAG_POLLUTANT_DAYS, [], [])\n",
    "\n",
    "    base_feats = [\"dow\",\"is_weekend\",\"month\",\"doy\",\"sin_doy\",\"cos_doy\",\"lat\",\"lon\"]\n",
    "    seq_feats = [c for c in df.columns if any(s in c for s in [\"_lag_\",\"_rollmean_\",\"_rollstd_\",\"_ewm_\"])]\n",
    "    feature_cols = base_feats + seq_feats\n",
    "\n",
    "    y = pd.to_numeric(df[\"aqi_next_24h\"], errors=\"coerce\")\n",
    "    X = df[[\"locationId\",\"date\"] + feature_cols].copy()\n",
    "    mask = y.notna()\n",
    "    for c in feature_cols:\n",
    "        mask &= X[c].notna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    return df, X, y, feature_cols\n",
    "\n",
    "def main():\n",
    "    df_all, X, y, feature_cols = build_dataset(INPUT_CSV, WEATHER_CSV)\n",
    "\n",
    "    # ---------- Time-based split (FIX) ----------\n",
    "    train_idx, val_idx = time_split_indices(X, y, frac_val=0.2)\n",
    "    X_train, y_train = X.loc[train_idx, :], y.loc[train_idx]\n",
    "    X_val,   y_val   = X.loc[val_idx, :],   y.loc[val_idx]\n",
    "\n",
    "    # ---------- Pipeline ----------\n",
    "    preproc = ColumnTransformer([(\"num\", \"passthrough\", feature_cols)], remainder=\"drop\")\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_depth=None, learning_rate=0.06, max_iter=600,\n",
    "        l2_regularization=1e-6, early_stopping=True,\n",
    "        validation_fraction=0.1, random_state=42\n",
    "    )\n",
    "    pipe = Pipeline([(\"prep\", preproc), (\"model\", model)])\n",
    "\n",
    "    # ---------- Hyperparameter tuning ----------\n",
    "    split_index = np.full(len(X), -1)\n",
    "    split_index[val_idx] = 0\n",
    "    ps = PredefinedSplit(split_index)\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__learning_rate\": loguniform(0.01, 0.2),\n",
    "        \"model__max_iter\": randint(400, 1400),\n",
    "        \"model__max_depth\": randint(3, 14),\n",
    "        \"model__l2_regularization\": loguniform(1e-7, 1e-2),\n",
    "        \"model__min_samples_leaf\": randint(10, 200)\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe, param_distributions=param_dist, n_iter=40,\n",
    "        cv=ps, scoring=\"neg_mean_absolute_error\", n_jobs=1,\n",
    "        random_state=42, verbose=1\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    best_pipe = search.best_estimator_\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "\n",
    "    # ---------- Validate tuned model ----------\n",
    "    pred_val = best_pipe.predict(X_val)\n",
    "    overall = regression_report(y_val.values, pred_val)\n",
    "    print(\"\\n=== Validation Metrics (tuned) ===\")\n",
    "    for k, v in overall.items():\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "    # ---------- Per-site bias calibration ----------\n",
    "    val_frame = X_val[[\"locationId\",\"date\"]].copy()\n",
    "    val_frame[\"y_true\"] = y_val.values\n",
    "    val_frame[\"y_pred\"] = pred_val\n",
    "    site_bias = (val_frame.groupby(\"locationId\")\n",
    "                          .apply(lambda g: (g[\"y_true\"] - g[\"y_pred\"]).mean())\n",
    "                          .to_dict())\n",
    "\n",
    "    def apply_site_bias(dfX, preds):\n",
    "        adj = preds.copy()\n",
    "        for i, loc in enumerate(dfX[\"locationId\"].values):\n",
    "            adj[i] = preds[i] + site_bias.get(loc, 0.0)\n",
    "        return adj\n",
    "\n",
    "    pred_val_adj = apply_site_bias(X_val, pred_val)\n",
    "    overall_adj = regression_report(y_val.values, pred_val_adj)\n",
    "    print(\"\\n=== Validation Metrics (tuned + per-site bias) ===\")\n",
    "    for k, v in overall_adj.items():\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "    # ---------- Save validation outputs ----------\n",
    "    out_val = X_val[[\"locationId\",\"date\"]].copy()\n",
    "    out_val[\"y_true\"] = y_val.values\n",
    "    out_val[\"y_pred\"] = pred_val\n",
    "    out_val[\"y_pred_biasadj\"] = pred_val_adj\n",
    "    out_val.to_csv(VAL_PREDS_CSV, index=False)\n",
    "    pd.DataFrame([overall]).to_csv(VAL_REPORT_CSV, index=False)\n",
    "\n",
    "    per_rows = []\n",
    "    for loc, g in out_val.groupby(\"locationId\"):\n",
    "        rep = regression_report(g[\"y_true\"].values, g[\"y_pred\"].values)\n",
    "        rep_bias = regression_report(g[\"y_true\"].values, g[\"y_pred_biasadj\"].values)\n",
    "        per_rows.append({\n",
    "            \"locationId\": loc, \"n\": len(g),\n",
    "            **{f\"raw_{k}\": v for k, v in rep.items()},\n",
    "            **{f\"biasadj_{k}\": v for k, v in rep_bias.items()},\n",
    "        })\n",
    "    pd.DataFrame(per_rows).to_csv(VAL_PER_SITE_CSV, index=False)\n",
    "\n",
    "    # ---------- Permutation importance ----------\n",
    "    feat_names = best_pipe.named_steps[\"prep\"].feature_names_in_\n",
    "    r = permutation_importance(\n",
    "        best_pipe, X_val, y_val, n_repeats=10, random_state=42,\n",
    "        n_jobs=1, scoring=\"neg_mean_absolute_error\"\n",
    "    )\n",
    "    importances = (pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"importance_mean\": r.importances_mean,\n",
    "        \"importance_std\": r.importances_std\n",
    "    }).sort_values(\"importance_mean\", ascending=False))\n",
    "    importances.to_csv(VAL_IMPORTANCES_CSV, index=False)\n",
    "    print(\"\\nTop 15 features:\")\n",
    "    print(importances.head(15))\n",
    "\n",
    "    # ---------- Retrain on ALL data and save ----------\n",
    "    best_pipe.fit(X, y)\n",
    "    joblib.dump({\"pipeline\": best_pipe, \"features\": feature_cols, \"site_bias\": site_bias}, MODEL_PATH)\n",
    "    print(f\"\\n[✓] saved tuned model → {MODEL_PATH}\")\n",
    "\n",
    "    # ---------- Predict next-day AQI (t -> t+1) ----------\n",
    "    X_latest = (X.sort_values(\"date\").groupby(\"locationId\", as_index=False).tail(1).copy())\n",
    "    preds_next = best_pipe.predict(X_latest[feat_names])\n",
    "    # FIXED brackets below\n",
    "    preds_next_adj = np.array([p + site_bias.get(loc, 0.0)\n",
    "                               for p, loc in zip(preds_next, X_latest[\"locationId\"])])\n",
    "    next_out = X_latest[[\"locationId\",\"date\"]].copy()\n",
    "    next_out[\"date\"] = next_out[\"date\"] + pd.Timedelta(days=1)\n",
    "    next_out[\"aqi_pred_next_24h\"] = preds_next\n",
    "    next_out[\"aqi_pred_next_24h_biasadj\"] = preds_next_adj\n",
    "    print(\"\\nNext-day AQI predictions (per location, raw & bias-adjusted):\")\n",
    "    print(next_out.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c374c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing locationId=924\n",
      "Chose date t = 2016-05-06 (first available with full features)\n",
      "Predicted AQI for t+1: 37.96 | bias-adjusted: 32.85\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf7f88c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "No testable row with ground truth on/after 2016-05-06 for site 924.\nTry a date >= 2016-03-20 and ensure there is data for the next day.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m No testable row with ground truth on/after 2016-05-06 for site 924.\nTry a date >= 2016-03-20 and ensure there is data for the next day.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f554be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
